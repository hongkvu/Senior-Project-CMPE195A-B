{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hongkvu/Senior-Project-CMPE195A-B/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrWxbd3scK4y",
        "outputId": "bfe463b0-0a06-4679-f568-2a753d51c8f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Nov 12 01:31:22 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P0    29W /  70W |   4222MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Naaamk5SqFbw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e096215-d4e2-4d87-d5b2-67053a02198c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "JxCZqgQ9ciC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e44aae9a-5c78-44ba-ba9f-9f346fdcb021"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.7/dist-packages (2.10.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.14.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.19.6)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (22.10.26)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (14.0.6)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.50.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (57.4.0)\n",
            "Requirement already satisfied: keras<2.11,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (21.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.21.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied: tensorboard<2.11,>=2.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (4.1.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.27.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow-gpu) (0.38.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow-gpu) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (2.14.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-gpu) (5.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-gpu) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow-gpu) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow-gpu) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow-gpu) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-gpu) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow-gpu) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-gpu) (3.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.12.1+cu111 (from versions: 0.4.1, 0.4.1.post2, 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.2.0+cpu, 1.2.0+cu92, 1.3.0, 1.3.0+cpu, 1.3.0+cu100, 1.3.0+cu92, 1.3.1, 1.3.1+cpu, 1.3.1+cu100, 1.3.1+cu92, 1.4.0, 1.4.0+cpu, 1.4.0+cu100, 1.4.0+cu92, 1.5.0, 1.5.0+cpu, 1.5.0+cu101, 1.5.0+cu92, 1.5.1, 1.5.1+cpu, 1.5.1+cu101, 1.5.1+cu92, 1.6.0, 1.6.0+cpu, 1.6.0+cu101, 1.6.0+cu92, 1.7.0, 1.7.0+cpu, 1.7.0+cu101, 1.7.0+cu110, 1.7.0+cu92, 1.7.1, 1.7.1+cpu, 1.7.1+cu101, 1.7.1+cu110, 1.7.1+cu92, 1.7.1+rocm3.7, 1.7.1+rocm3.8, 1.8.0, 1.8.0+cpu, 1.8.0+cu101, 1.8.0+cu111, 1.8.0+rocm3.10, 1.8.0+rocm4.0.1, 1.8.1, 1.8.1+cpu, 1.8.1+cu101, 1.8.1+cu102, 1.8.1+cu111, 1.8.1+rocm3.10, 1.8.1+rocm4.0.1, 1.9.0, 1.9.0+cpu, 1.9.0+cu102, 1.9.0+cu111, 1.9.0+rocm4.0.1, 1.9.0+rocm4.1, 1.9.0+rocm4.2, 1.9.1, 1.9.1+cpu, 1.9.1+cu102, 1.9.1+cu111, 1.9.1+rocm4.0.1, 1.9.1+rocm4.1, 1.9.1+rocm4.2, 1.10.0, 1.10.0+cpu, 1.10.0+cu102, 1.10.0+cu111, 1.10.0+cu113, 1.10.0+rocm4.0.1, 1.10.0+rocm4.1, 1.10.0+rocm4.2, 1.10.1, 1.10.1+cpu, 1.10.1+cu102, 1.10.1+cu111, 1.10.1+cu113, 1.10.1+rocm4.0.1, 1.10.1+rocm4.1, 1.10.1+rocm4.2, 1.10.2, 1.10.2+cpu, 1.10.2+cu102, 1.10.2+cu111, 1.10.2+cu113, 1.10.2+rocm4.0.1, 1.10.2+rocm4.1, 1.10.2+rocm4.2, 1.11.0, 1.11.0+cpu, 1.11.0+cu102, 1.11.0+cu113, 1.11.0+cu115, 1.11.0+rocm4.3.1, 1.11.0+rocm4.5.2, 1.12.0, 1.12.0+cpu, 1.12.0+cu102, 1.12.0+cu113, 1.12.0+cu116, 1.12.0+rocm5.0, 1.12.0+rocm5.1.1, 1.12.1, 1.12.1+cpu, 1.12.1+cu102, 1.12.1+cu113, 1.12.1+cu116, 1.12.1+rocm5.0, 1.12.1+rocm5.1.1, 1.13.0, 1.13.0+cpu, 1.13.0+cu116, 1.13.0+cu117, 1.13.0+cu117.with.pypi.cudnn, 1.13.0+rocm5.1.1, 1.13.0+rocm5.2)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for torch==1.12.1+cu111\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-gpu\n",
        "!pip install einops\n",
        "!pip install torch==1.12.1+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR8TFxtrTOc1"
      },
      "source": [
        "## What is Einops?\n",
        "Einops, an abbreviation of Einstein-Inspired Notation for operations is an open-source python framework for writing deep learning code in a new and better way. Einops provides us with new notation & new operations. It is  a flexible and powerful tool to ensure code readability and reliability with minimalist yet powerful API.\n",
        "\n",
        "Supports numpy, pytorch, tensorflow, jax, and others.\n",
        "\n",
        "Source: https://analyticsindiamag.com/reinventing-deep-learning-operation-via-einops/\n",
        "##Example:\n",
        "from einops import rearrange, reduce, repeat\n",
        "### Rearrange elements according to the pattern\n",
        "- output_tensor = rearrange(input_tensor, 't b c -> b c t')\n",
        "\n",
        "### Combine rearrangement and reduction\n",
        "- output_tensor = reduce(input_tensor, 'b c (h h2) (w w2) -> b h w c', 'mean', h2=2, w2=2)\n",
        "\n",
        "### Copy along a new axis \n",
        "- output_tensor = repeat(input_tensor, 'h w -> h w c', c=3)\n",
        "\n",
        "### Example given for einops, but code in other frameworks is almost identical  \n",
        "````\n",
        "from torch.nn import Sequential, Conv2d, MaxPool2d, Linear, ReLU\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "model = Sequential(\n",
        "    Conv2d(3, 6, kernel_size=5),\n",
        "    MaxPool2d(kernel_size=2),\n",
        "    Conv2d(6, 16, kernel_size=5),\n",
        "    MaxPool2d(kernel_size=2),\n",
        "    # flattening\n",
        "    Rearrange('b c h w -> b (c h w)'),  \n",
        "    Linear(16*5*5, 120), \n",
        "    ReLU(),\n",
        "    Linear(120, 10), \n",
        ")\n",
        "````\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vicqxW7wnfwX"
      },
      "source": [
        "CoAtNet src code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "xRoadqXyOFlI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e183ba16-1ad9-4ea5-f1d0-4824a91dc5fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1000]) 17789624\n",
            "torch.Size([1, 1000]) 33170624\n",
            "torch.Size([1, 1000]) 55767564\n",
            "torch.Size([1, 1000]) 117724480\n",
            "torch.Size([1, 1000]) 203960368\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "# \tnn.Conv2d : Applies a 2D convolution over an input signal composed of several input planes.\n",
        "\n",
        "def conv_3x3_bn(inp, oup, image_size, downsample=False):\n",
        "    stride = 1 if downsample == False else 2\n",
        "    return nn.Sequential(                               # build neural networks \n",
        "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),  # slide a matrix or filter over 2D data\n",
        "        #Channel - The colors(?) that transmit the actual information to the receiver. Usually RGB. The colors in an image are created by a mix of Red, Green, and Blue.\n",
        "        #inp = number of channels in the input image.\n",
        "        #oup = number of channels produced by the convolution\n",
        "        #3 = kernel size (3x3 in this case). Kernel is the filter used to extract the features from the images. The matrix that slides over the 2D data. (In 3D, a filter is a collection of kernels)\n",
        "        #stride = how many pixels the kernel will shift over the image.\n",
        "        #bias - is like the b parameter in y = mx + b -- Helps model fit the training set properly\n",
        "        nn.BatchNorm2d(oup),  # Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) \n",
        "        # Batch normalization normalizes the activations of the network between layers in batches so that the batches have a mean of 0 and a variance of 1\n",
        "        nn.GELU() # The Gaussian Error Linear Unit is an activation function.\n",
        "        # The GELU activation function is xΦ(x) \n",
        "        # GELU consistently achieves the lowest test error rate,\n",
        "        # posing as a promising alternative to ReLU and ELU for a neural network’s activation function.\n",
        "    )\n",
        "\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn, norm):\n",
        "        super().__init__()    # initializes the parent class object into the child class\n",
        "        # __init__(): allows the class to initialize the attributes of the class\n",
        "        # super(): function allows us to avoid using the base class name explicitly.\n",
        "        self.norm = norm(dim)\n",
        "        # self represents the instance of the class. By using the “self”  we can access the attributes and methods of the class in python. It binds the attributes with the given arguments.\n",
        "        # norm is what is generally used to evaluate the error of a model\n",
        "        # norm returns the matrix norm or vector norm of a given tensor\n",
        "        # dim – Specifies which dimension or dimensions of input to calculate the norm across. \n",
        "        # If dim is None, the norm will be calculated across all dimensions of input. \n",
        "        # If the norm type indicated by p does not support the specified number of dimensions, an error will occur.\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "        #**kwargs allow to pass multiple arguments or keyword arguments to a function. \n",
        "\n",
        "\n",
        "# SE Blocks - the Squeeze-Excite process:a modular implementation that can be easily implemented to most Deep CNNs.\n",
        "# The main idea of an SE Block is: Assign each channel of a feature map a different weightage (excitation) based on how important each channel is (squeeze).\n",
        "# crucial to the gains in performance\n",
        "class SE(nn.Module):\n",
        "    def __init__(self, inp, oup, expansion=0.25):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)                 #Sets the output size of the pooling layer to 1x1\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(oup, int(inp * expansion), bias=False),   #applies a linear transformation\n",
        "            nn.GELU(),        #applies the Gaussian Error Linear Units function\n",
        "            nn.Linear(int(inp * expansion), oup, bias=False),\n",
        "            nn.Sigmoid()      # An activation function that takes a value and turns it into a value between 0 and 1 to predict probabilities.\n",
        "        )\n",
        "\n",
        "    def forward(self, x):               # define the flow of forward process \n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c) # Squeeze - perform Global Average Pooling\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim), #applies a linear transformation\n",
        "            nn.GELU(),       #applies the Gaussian Error Linear Units function\n",
        "            nn.Dropout(dropout),  # takes in the dropout rate\n",
        "            # to Prevent Neural Networks from Overfitting because it increase accuracy\n",
        "            nn.Linear(hidden_dim, dim),   #applies a linear transformation\n",
        "            nn.Dropout(dropout) # takes in the dropout rate\n",
        "        )\n",
        "    # define the flow of forward process \n",
        "    def forward(self, x):             # x in the forward() method is the input vector.\n",
        "        return self.net(x)\n",
        "\n",
        "# A MBConv is a Inverted Linear BottleNeck layer with Depth-Wise Separable Convolution.\n",
        "class MBConv(nn.Module):\n",
        "    def __init__(self, inp, oup, image_size, downsample=False, expansion=4):\n",
        "        super().__init__()\n",
        "        self.downsample = downsample    # reducing the sampling rate of a signal.\n",
        "        # A reduction of the feature maps sizes(downsampling) \n",
        "        # as we move through the network enables the possibility of reducing the spatial resolution of the feature map.\n",
        "        stride = 1 if self.downsample == False else 2\n",
        "        # Stride is a parameter of the neural network's filter that modifies the amount of movement over the image or video. For example, if a neural network's stride is set to 1, the filter will move one pixel, or unit, at a time.\n",
        "        #If stride = 2, the filter will move two pixels.\n",
        "        hidden_dim = int(inp * expansion)   # Number of hidden layers self.\n",
        "        # The hidden dimension is basically the number of nodes in each layer (like in the Multilayer Perceptron for example) \n",
        "        # The embedding size tells the size of the feature vector (the model uses embedded words as input)\n",
        "\n",
        "        if self.downsample:\n",
        "            self.pool = nn.MaxPool2d(3, 2, 1)   # Applies a 2D max pooling over an input signal composed of several input planes.\n",
        "            # kernel_size=3, padding=2, dilation=1\n",
        "            # kernel_size – the size of the window to take a max over\n",
        "            # stride – the stride of the window. Default value is kernel_size\n",
        "            # padding – implicit zero padding to be added on both sides\n",
        "            # dilation – a parameter that controls the stride of elements in the window\n",
        "            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
        "            # slide a matrix or filter over 2D data\n",
        "\n",
        "        if expansion == 1:\n",
        "            self.conv = nn.Sequential(\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride,    # build neural networks that slide a matrix or filter over 2D data\n",
        "                          1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),                     # Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) \n",
        "                nn.GELU(),                                      #applies the Gaussian Error Linear Units function\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False), # build neural networks that slide a matrix or filter over 2D data\n",
        "                nn.BatchNorm2d(oup),                             # Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) \n",
        "            )\n",
        "        else:\n",
        "            self.conv = nn.Sequential(\n",
        "                # pw\n",
        "                # down-sample in the first conv\n",
        "                nn.Conv2d(inp, hidden_dim, 1, stride, 0, bias=False),    # build neural networks that slide a matrix or filter over 2D data\n",
        "                nn.BatchNorm2d(hidden_dim),                              # Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension)\n",
        "                nn.GELU(),\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, 1, 1,\n",
        "                          groups=hidden_dim, bias=False),                # build neural networks that slide a matrix or filter over 2D data\n",
        "                nn.BatchNorm2d(hidden_dim),                              # Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension)\n",
        "                nn.GELU(),                                               #applies the Gaussian Error Linear Units function\n",
        "                SE(inp, hidden_dim),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),         # build neural networks that slide a matrix or filter over 2D data\n",
        "                nn.BatchNorm2d(oup),                                     # Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension)\n",
        "            ) \n",
        "        \n",
        "        self.conv = PreNorm(inp, self.conv, nn.BatchNorm2d)\n",
        "\n",
        "    def forward(self, x):           # define the flow of forward process\n",
        "        if self.downsample:\n",
        "            return self.proj(self.pool(x)) + self.conv(x)\n",
        "        else:\n",
        "            return x + self.conv(x)\n",
        "\n",
        "# multihead attention: The goal is to take an average over the features of multiple elements.\n",
        "# Allows the model to jointly attend to information from different representation subspaces\n",
        "# Attention models (or mechanisms) are neural network input processing strategies \n",
        "# that allow the network to focus on specific parts of a complicated input one by one until the entire dataset is categorized\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, dropout=0.):\n",
        "      # dim_head – Total dimension of the model.\n",
        "      # heads – Number of parallel attention heads. Note that dim_head will be split across num_heads (i.e. each head will have dimension dim_head // heads).\n",
        "      # dropout – Dropout probability on attn_output_weights. Default: 0.0 (no dropout).\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        project_out = not (heads == 1 and dim_head == inp)\n",
        "\n",
        "        self.ih, self.iw = image_size\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        # parameter table of relative position bias\n",
        "        self.relative_bias_table = nn.Parameter(\n",
        "            torch.zeros((2 * self.ih - 1) * (2 * self.iw - 1), heads))\n",
        "\n",
        "        coords = torch.meshgrid((torch.arange(self.ih), torch.arange(self.iw)))\n",
        "        coords = torch.flatten(torch.stack(coords), 1)\n",
        "        relative_coords = coords[:, :, None] - coords[:, None, :]\n",
        "\n",
        "        relative_coords[0] += self.ih - 1\n",
        "        relative_coords[1] += self.iw - 1\n",
        "        relative_coords[0] *= 2 * self.iw - 1\n",
        "        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n",
        "        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n",
        "        self.register_buffer(\"relative_index\", relative_index)\n",
        "\n",
        "        self.attend = nn.Softmax(dim=-1)\n",
        "        self.to_qkv = nn.Linear(inp, inner_dim * 3, bias=False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, oup),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: rearrange(\n",
        "            t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
        "\n",
        "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "\n",
        "        # Use \"gather\" for more efficiency on GPUs\n",
        "        relative_bias = self.relative_bias_table.gather(\n",
        "            0, self.relative_index.repeat(1, self.heads))\n",
        "        relative_bias = rearrange(\n",
        "            relative_bias, '(h w) c -> 1 c h w', h=self.ih*self.iw, w=self.ih*self.iw)\n",
        "        dots = dots + relative_bias\n",
        "\n",
        "        attn = self.attend(dots)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n",
        "# Transformer architecture implements an encoder-decoder structure without recurrence and convolutions in order to generate an output.\n",
        "# A transformer is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data.\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, downsample=False, dropout=0.):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(inp * 4)\n",
        "\n",
        "        self.ih, self.iw = image_size\n",
        "        self.downsample = downsample\n",
        "\n",
        "        if self.downsample:\n",
        "            self.pool1 = nn.MaxPool2d(3, 2, 1)\n",
        "            self.pool2 = nn.MaxPool2d(3, 2, 1)\n",
        "            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
        "\n",
        "        self.attn = Attention(inp, oup, image_size, heads, dim_head, dropout)     # attention technique\n",
        "        #inp = number of channels in the input image.\n",
        "        #oup = number of channels produced by the convolution\n",
        "        #dim = dimension per head\n",
        "        self.ff = FeedForward(oup, hidden_dim, dropout)\n",
        "        # feed-forward neural networks: used in practice to transform the attention vectors into a digestible form by the next encoder/decoder block\n",
        "\n",
        "        self.attn = nn.Sequential(\n",
        "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
        "            PreNorm(inp, self.attn, nn.LayerNorm),\n",
        "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
        "        )\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            Rearrange('b c ih iw -> b (ih iw) c'),  # # concatenated images along horizontal axis\n",
        "            # einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors. \n",
        "            # This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze, stack, concatenate and other operations.\n",
        "            PreNorm(oup, self.ff, nn.LayerNorm),\n",
        "            # nn.LayerNorm - Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization\n",
        "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)   # space-to-depth operation\n",
        "            # ih, iw = image_size\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.downsample:\n",
        "            x = self.proj(self.pool1(x)) + self.attn(self.pool2(x))\n",
        "        else:\n",
        "            x = x + self.attn(x)\n",
        "        x = x + self.ff(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class CoAtNet(nn.Module):\n",
        "    def __init__(self, image_size, in_channels, num_blocks, channels, num_classes=1000, block_types=['C', 'C', 'T', 'T']):\n",
        "        super().__init__()\n",
        "        ih, iw = image_size\n",
        "        block = {'C': MBConv, 'T': Transformer}\n",
        "\n",
        "        self.s0 = self._make_layer(\n",
        "            conv_3x3_bn, in_channels, channels[0], num_blocks[0], (ih // 2, iw // 2))\n",
        "        self.s1 = self._make_layer(\n",
        "            block[block_types[0]], channels[0], channels[1], num_blocks[1], (ih // 4, iw // 4))\n",
        "        self.s2 = self._make_layer(\n",
        "            block[block_types[1]], channels[1], channels[2], num_blocks[2], (ih // 8, iw // 8))\n",
        "        self.s3 = self._make_layer(\n",
        "            block[block_types[2]], channels[2], channels[3], num_blocks[3], (ih // 16, iw // 16))\n",
        "        self.s4 = self._make_layer(\n",
        "            block[block_types[3]], channels[3], channels[4], num_blocks[4], (ih // 32, iw // 32))\n",
        "\n",
        "        self.pool = nn.AvgPool2d(ih // 32, 1)\n",
        "        self.fc = nn.Linear(channels[-1], num_classes, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.s0(x)\n",
        "        x = self.s1(x)\n",
        "        x = self.s2(x)\n",
        "        x = self.s3(x)\n",
        "        x = self.s4(x)\n",
        "\n",
        "        x = self.pool(x).view(-1, x.shape[1])\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def _make_layer(self, block, inp, oup, depth, image_size):\n",
        "        layers = nn.ModuleList([])\n",
        "        for i in range(depth):\n",
        "            if i == 0:\n",
        "                layers.append(block(inp, oup, image_size, downsample=True))\n",
        "            else:\n",
        "                layers.append(block(oup, oup, image_size))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def coatnet_0():\n",
        "    num_blocks = [2, 2, 3, 5, 2]            # L\n",
        "    channels = [64, 96, 192, 384, 768]      # D\n",
        "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n",
        "\n",
        "\n",
        "def coatnet_1():\n",
        "    num_blocks = [2, 2, 6, 14, 2]           # L\n",
        "    channels = [64, 96, 192, 384, 768]      # D\n",
        "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n",
        "\n",
        "\n",
        "def coatnet_2():\n",
        "    num_blocks = [2, 2, 6, 14, 2]           # L\n",
        "    channels = [128, 128, 256, 512, 1026]   # D\n",
        "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n",
        "\n",
        "\n",
        "def coatnet_3():\n",
        "    num_blocks = [2, 2, 6, 14, 2]           # L\n",
        "    channels = [192, 192, 384, 768, 1536]   # D\n",
        "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n",
        "\n",
        "\n",
        "def coatnet_4():\n",
        "    num_blocks = [2, 2, 12, 28, 2]          # L\n",
        "    channels = [192, 192, 384, 768, 1536]   # D\n",
        "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n",
        "\n",
        "\n",
        "#PyTorch doesn't have a function to calculate the total number of parameters as Keras does, \n",
        "#but it's possible to sum the number of elements for every parameter group\n",
        "#trainable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    img = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "    net = coatnet_0()\n",
        "    out = net(img)\n",
        "    print(out.shape, count_parameters(net))\n",
        "\n",
        "    net = coatnet_1()\n",
        "    out = net(img)\n",
        "    print(out.shape, count_parameters(net))\n",
        "\n",
        "    net = coatnet_2()\n",
        "    out = net(img)\n",
        "    print(out.shape, count_parameters(net))\n",
        "\n",
        "    net = coatnet_3()\n",
        "    out = net(img)\n",
        "    print(out.shape, count_parameters(net))\n",
        "\n",
        "    net = coatnet_4()\n",
        "    out = net(img)\n",
        "    print(out.shape, count_parameters(net))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDExYK-ugMyv"
      },
      "source": [
        "Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4JMZx37gRvV"
      },
      "source": [
        "Pytorch is an open source machine learning framework that accelerates the path from research prototyping to production deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS_TSaeThTLV"
      },
      "source": [
        "- Pytorch.nn: The nn package defines a set of Modules, which you can think of as a neural network layer that has produces output from input and may have some trainable weights.\n",
        "  - nn documents: https://pytorch.org/docs/stable/nn.html\n",
        "  - Container \n",
        "    - nn.Module: Base class for all neural network modules.\n",
        "      - Example: \n",
        "            class Model(nn.Module):\n",
        "              def __init__(self):\n",
        "              super().__init__()\n",
        "              self.conv1 = nn.Conv2d(1, 20, 5)\n",
        "              self.conv2 = nn.Conv2d(20, 20, 5)\n",
        "      - Modules can also contain other Modules, allowing to nest them in a tree structure: \n",
        "            module (Module) – child module to be added to the module\n",
        "    - nn.Sequential: A sequential container.\n",
        "      - Class torch.nn.Sequential(*args) \n",
        "      - Modules will be added to it in the order they are passed in the constructor. It accepts any input and forwards it to the first module it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\n",
        "      - Example: \n",
        "            model = nn.Sequential(\n",
        "                      nn.Conv2d(1,20,5),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(20,64,5),\n",
        "                      nn.ReLU()\n",
        "                    )\n",
        "      -> Run `model`: input will first be passed to `Conv2d(1,20,5)` -> output of `Conv2d(1,20,5)` is used as input to the first `ReLU` -> output of the first `ReLU` becomes input for `Conv2d(20,64,5)` -> output of `Conv2d(20,64,5)` is used as input to the second `ReLU`.\n",
        "  - Convolution Layers (https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)\n",
        "    - nn.Conv2d: Applies a 2D convolution over an input signal composed of several input planes.\n",
        "    - Constructor: torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
        "      - stride: stride for the cross-correlation, a single number or a tuple.\n",
        "      - padding: amount of padding applied to the input. \n",
        "      - dilation: spacing between the kernel points.\n",
        "      - groups: connections between inputs and outputs.\n",
        "        - 1: all inputs are convolved to all outputs.\n",
        "        - 2: the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels and producing half the output channels, and both subsequently concatenated.\n",
        "        - in_channels: each input channel is convolved with its own set of filters  \n",
        "  - Non-linear Activations\n",
        "    - nn.ReLU: Applies the rectified linear unit function element-wise: \n",
        "            ReLU(x) = (x)+ = max(0,x)\n",
        "      - Constructor: CLASStorch.nn.ReLU(inplace=False)\n",
        "        - inplace – can optionally do the operation in-place. Default: False\n",
        "  - Normalization Layers\n",
        "    - nn.BatchNorm2d: Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension).\n",
        "    - Constructor: CLASS torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)\n",
        "      - num_features – CC from an expected input of size (N, C, H, W)(N,C,H,W)\n",
        "      - eps – a value added to the denominator for numerical stability.\n",
        "        - Default: 1e-5\n",
        "      - momentum – the value used for the running_mean and running_var computation. \n",
        "        - None for cumulative moving average \n",
        "        - Default: 0.1\n",
        "      - affine – a boolean value. \n",
        "        - True: this module has learnable affine parameters. Default: True\n",
        "      - track_running_stats – a boolean value. Default: True\n",
        "        - True: this module tracks the running mean and variance,\n",
        "        - False: this module does not track such statistics \n",
        "    - read: https://arxiv.org/abs/1502.03167\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0Z5iSSDiYwF"
      },
      "source": [
        "- torch.randn (*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)\n",
        "  - Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (standard normal distribution).\n",
        "  - Parameters\n",
        "    - size - sequence of integers defining the size of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.\n",
        "    - out (Tensor, optional) – the output tensor.\n",
        "    - dtype (torch.dtype, optional) – the desired data type of returned tensor. \n",
        "      - Default: None - uses a global default.\n",
        "    - layout (torch.layout, optional) – the desired layout of returned Tensor. \n",
        "      - Default: torch.strided.\n",
        "    - device (torch.device, optional) \n",
        "      - the desired device of returned tensor. \n",
        "      - Default: None - uses the current device for the default tensor type. device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
        "    - requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "8UvN2ZKQgGLl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98ccdcfb-c987-49d0-e778-e0df8d2df816"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[-0.6892,  1.6593, -0.6723,  ..., -0.7964, -0.6858, -0.6469],\n",
            "          [-0.1663,  0.7546, -0.6215,  ...,  1.2269, -0.1254,  0.3593],\n",
            "          [ 0.3749,  0.4360, -0.2643,  ...,  0.5677, -0.4663,  1.3145],\n",
            "          ...,\n",
            "          [ 1.4840, -0.2628, -0.2105,  ..., -0.0256, -1.0311,  1.6852],\n",
            "          [ 1.5459, -0.1725,  0.3872,  ..., -0.2350, -0.2636, -0.2294],\n",
            "          [ 0.8764,  0.8011, -0.2702,  ...,  0.1793, -0.3699,  1.8846]],\n",
            "\n",
            "         [[-2.1476, -0.1698,  0.7078,  ..., -0.8838, -1.0165,  1.0427],\n",
            "          [ 1.9652,  1.0244,  1.4998,  ..., -1.7821, -1.0843, -0.0440],\n",
            "          [ 1.0335, -0.5021,  0.9977,  ..., -0.4540,  2.3891,  0.0600],\n",
            "          ...,\n",
            "          [ 1.2156, -0.7336, -1.7813,  ..., -1.5937,  0.1017, -1.3230],\n",
            "          [ 0.7952,  0.1637, -2.1014,  ...,  0.6433, -0.3839, -1.0443],\n",
            "          [ 0.0882, -1.0985,  0.3659,  ..., -0.1076, -1.9895,  0.7055]],\n",
            "\n",
            "         [[ 0.1412,  1.4194, -0.0824,  ...,  0.1973, -0.8483,  0.4820],\n",
            "          [-2.3770, -0.9657, -2.1320,  ..., -1.0615, -0.2366,  1.2266],\n",
            "          [ 0.2418,  1.5804,  0.5521,  ...,  0.4157, -0.4499,  0.8617],\n",
            "          ...,\n",
            "          [ 0.9897, -1.5487,  1.9625,  ...,  0.2478,  0.0826,  1.2687],\n",
            "          [ 0.3131,  0.9737, -0.1209,  ..., -0.4491, -0.7279,  1.7307],\n",
            "          [ 0.2635,  0.0237,  0.2508,  ...,  0.8148,  1.2624, -0.3217]]]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "import torch\n",
        "x=torch.randn(1,3,224,224)\n",
        "print(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "KUy1nOI4ES74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b00dd394-12e4-461e-e29c-003a32c16440"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4727, -0.3124,  1.5685, -2.5521],\n",
            "        [-0.7375, -0.0254,  1.4065, -1.2235]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "import torch\n",
        "x=torch.randn(2,4)\n",
        "print(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "NsDCNPO_K8eJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8547ca56-1c0f-4483-be58-b8617329ecd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1734,  0.3090, -0.3576,  1.3584,  0.6911],\n",
            "         [-0.8926, -0.4250,  0.7204, -1.2492,  0.7899],\n",
            "         [-0.7864, -0.0214,  2.2198,  2.2587, -1.5498],\n",
            "         [-0.2459,  0.8235,  0.3664, -0.0872,  0.4401]],\n",
            "\n",
            "        [[ 0.5645, -0.4481, -0.4081,  0.9423, -2.5649],\n",
            "         [ 1.6812,  1.2535,  0.5351,  0.0147, -0.6321],\n",
            "         [ 0.1570,  0.9701,  1.3290,  0.1513,  1.3401],\n",
            "         [ 0.0963, -0.0307,  0.5578,  0.8949, -0.1009]]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "import torch\n",
        "x=torch.randn(2,4,5)\n",
        "print(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmIsfZytnKXC"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "def conv_3x3_bn(inp, oup, image_size, downsample=False): stride = 1 if downsample == False else 2\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.GELU()\n",
        "    )\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "**nn.Sequential()**\n",
        "\n",
        "Class that allows us to build neural networks on the fly without having to define an explicit class.\n",
        "\n",
        "  **nn.Conv2d(inp, oup, 3, stride, 1, bias=False)**\n",
        "\n",
        "  2D Convolution - We slide a matrix or filter over 2D data (an image turned into a grid of numbers) and perform element-wise multiplication with the data, then sum up the multiplication result to produce an output. We move the filter in strides until we get the final output matrix of the 2D convolution operation.\n",
        "\n",
        "  Channel - The colors(?) that transmit the actual information to the receiver. Usually RGB. The colors in an image are created by a mix of Red, Green, and Blue.\n",
        "\n",
        "  inp = number of channels in the input image. \n",
        "\n",
        "  oup = number of channels produced by the convolution\n",
        "\n",
        "  3 = kernel size (3x3 in this case). Kernel is the filter used to extract the features from the images. The matrix that slides over the 2D data. (In 3D, a filter is a collection of kernels)\n",
        "\n",
        "  stride = how many pixels the kernel will shift over the image.\n",
        "  \n",
        "  bias - is like the *b* parameter in *y = mx + b* -- Helps model fit the training set properly\n",
        "\n",
        "  **nn.BatchNorm2d(oup)** \n",
        "  is needed to normalize the output so the data will be on the same scale. If one feature is too large, this feature will drown out the smaller feature. During gradient descent, the neural network will have to make a large update to one weight compared to the other weight. It can cause the gradient descent trajectory to oscillate back and forth, thus taking more steps to reach the minimum.\n",
        "\n",
        "  **nn.GELU**\n",
        "\n",
        "  Gaussian Error Linear Units function. An activation function like sigmoid and RELU\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "```\n",
        "class SE(nn.Module):\n",
        "    def __init__(self, inp, oup, expansion=0.25):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(oup, int(inp * expansion), bias=False),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(int(inp * expansion), oup, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "```\n",
        "\n",
        "**nn.AdaptiveAvgPool2d(1)**\n",
        "\n",
        "Sets the output size of the pooling layer to the specified size. (1x1 in this case) The stride and kernel-size are automatically selected to get this output size.\n",
        "\n",
        "Pooling compresses and generalizes the features in the feature map.\n",
        "\n",
        "**nn.Linear()**\n",
        "\n",
        "**nn.Sigmoid()**\n",
        "\n",
        "An activation function that takes a value and turns it into a value between 0 and 1 to predict probabilities.\n",
        "  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3_0uboVgIjK"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "4R9urY7HPJFr"
      },
      "outputs": [],
      "source": [
        "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "wjs9-UoHgMuy"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim #optimizer, will hold the current state and will update the parameters based on the computed gradients\n",
        "import torch\n",
        "import torch.nn as nn #neural network framework\n",
        "import torch.nn.parallel \n",
        "import torch.utils.data\n",
        "import torch.utils.data.distributed\n",
        "import torchvision.transforms as transforms #common image transformation\n",
        "from torch.autograd import Variable #automatic differentiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTtlySbjuYo1"
      },
      "source": [
        "Set global parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "d0-0c34UuT6c"
      },
      "outputs": [],
      "source": [
        "modellr = 1e-4\n",
        "BATCH_SIZE = 16 #the dataset is divided into batches, each of 16 \n",
        "EPOCHS = 10 #number of times the entire dataset is passed through the neural network\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')#the GPU device that operations will be on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSQv_16ouUZJ"
      },
      "source": [
        "Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "CgakdXjSuUED"
      },
      "outputs": [],
      "source": [
        "#modifying images (resize, convert, normalize)  \n",
        "transform = transforms.Compose([ #chain the transformation together\n",
        "    transforms.Resize((224, 224)), #resize image\n",
        "    transforms.ToTensor(), #convert image to a tensor (matrix)\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) #normalize with (mean, standard deviation)\n",
        "\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywyF1Q0gutu7"
      },
      "source": [
        "Fetch the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "6F6o8vK8heGT"
      },
      "outputs": [],
      "source": [
        "# coding:utf8\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils import data\n",
        "from torchvision import transforms as T\n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "# Labels = {'alpine_sea_holly': 0, 'alstroemeria': 1, 'amaranth': 2, 'amaryllis': 3, 'ammi': 4, 'anemone': 5, \n",
        "#           'anthurium': 6, 'antirrhinum': 7, 'artichoke': 8, 'asian virginsbower': 9, 'aster': 10,\n",
        "#           'astilbe': 11, 'astrantia': 12, 'ball moss': 13, 'ballon flower': 14, 'barbeton daisy': 15,\n",
        "#           'bird of paradise': 16, 'black-eye': 17, 'bluebell': 18, 'bougainvillea': 19, 'bouvardia': 20,\n",
        "#           'brassica': 21, 'buttercup': 22, 'california poppy': 23, 'calla': 24, 'canna lily': 25,\n",
        "#           'carnation': 26, 'celosia': 27, 'chamelaucium': 28, 'chamomile': 29, 'chrysanthemum': 30,\n",
        "#           'colts_foot': 31, 'columbine': 32, 'cotton': 33, 'cowslip': 34, 'craspedia': 35,\n",
        "#           'crocus': 36, 'cyclamen': 37, 'daffodil': 38, 'dahlia': 39, 'daisy':40,\n",
        "#           'dandelion':41, 'delphinium': 42, 'eryngium': 43, 'eustoma': 44, 'forget-me-not': 45,\n",
        "#           'foxglove': 46, 'frangipani': 47, 'freesia': 48, 'fritillary': 49, 'garden phlox': 50,\n",
        "#           'gerbera': 51, 'gladiolus': 52, 'globe thistle': 53, 'gypsophila': 54, 'hyacinth': 55,\n",
        "#           'hydrangea': 56, 'hypericum': 57, 'iris': 58, 'lavender': 59, 'leucadendron': 60,\n",
        "#           'leucospermum': 61, 'lilac': 62, 'lily': 63, 'lily_of_the_valley': 64, 'lily_valley': 65,\n",
        "#           'limonium': 66, 'lotus': 67, 'love in the mist': 68, 'mattiola': 69, 'mimosa': 70,\n",
        "#           'morning glory': 71, 'muscari': 72, 'narcissus': 73, 'nelumbo nucifera': 74, 'orchid': 75,\n",
        "#           'ornithogalum': 76, 'oxeye daisy': 77, 'oxypetalum': 78, 'ozotamnus': 79, 'pansy': 80,\n",
        "#           'passion flower': 81, 'peony': 82, 'petunia': 83, 'poinsettia': 84, 'protea': 85,\n",
        "#           'ranunculus': 86, 'rose': 87, 'sedum': 88, 'silverbush': 89, 'skimmia': 90,\n",
        "#           'snowdrop': 91, 'solidago': 92, 'strelitzia': 93, 'sunflower': 94, 'tigerlily': 95,\n",
        "#           'trachelium': 96, 'tropical white morning-glory': 97, 'tulip': 98, 'veronica': 99, 'water lily': 100,\n",
        "#           'willow': 101, 'windflower': 102}\n",
        "\n",
        "# Labels = {\n",
        "#      'tulip': 0, 'sunflower': 1, 'rose': 2, 'dandelion': 3, 'daisy':4\n",
        "# }\n",
        "Labels = {\n",
        "     'daisy': 0, 'dandelion': 1, 'rose': 2, 'sunflower': 3, 'tulip':4\n",
        "}\n",
        " \n",
        "class FlowerData (data.Dataset):\n",
        " \n",
        "    def __init__(self, root, transforms=None, train=True, test=False):\n",
        "        \"\"\"\n",
        "        Main objective: to obtain the addresses of all pictures and divide the data according to training, verification and test\n",
        "        \"\"\"\n",
        "        self.test = test\n",
        "        self.transforms = transforms\n",
        " \n",
        "        if self.test:\n",
        "            imgs = [os.path.join(root, img) for img in os.listdir(root)]\n",
        "            self.imgs = imgs\n",
        "        else:\n",
        "            imgs_labels = [os.path.join(root, img) for img in os.listdir(root)]\n",
        "            imgs = []\n",
        "            for imglable in imgs_labels:\n",
        "                for imgname in os.listdir(imglable):\n",
        "                    imgpath = os.path.join(imglable, imgname)\n",
        "                    imgs.append(imgpath)\n",
        "            trainval_files, val_files = train_test_split(imgs, test_size=0.3, random_state=42)\n",
        "            if train:\n",
        "                self.imgs = trainval_files\n",
        "            else:\n",
        "                self.imgs = val_files\n",
        " \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Returns the data of one picture at a time\n",
        "        \"\"\"\n",
        "        img_path = self.imgs[index]\n",
        "        img_path=img_path.replace(\"\\\\\",'/')\n",
        "        if self.test:\n",
        "            label = -1\n",
        "        else:\n",
        "            labelname = img_path.split('/')[-2]\n",
        "            label = Labels[labelname]\n",
        "        data = Image.open(img_path).convert('RGB')\n",
        "        data = self.transforms(data)\n",
        "        return data, label\n",
        " \n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split-data\n",
        "!pip install split-folders\n",
        "import splitfolders\n",
        "import pathlib\n",
        "data = \"drive/My Drive/195A+BSeniorProjectGroupWorks/Datasets/Flower_Dataset/flowers\"\n",
        "data_splitted = \"drive/My Drive/195A+BSeniorProjectGroupWorks/Datasets/Flower_Dataset/flowers_train_test_set\"\n",
        "splitfolders.ratio(data, data_splitted, seed=1337, ratio=(.8, .1, .1)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRLFjDcvIlPs",
        "outputId": "b32085d0-79e5-4c86-f518-9547349d6f5e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: split-folders in /usr/local/lib/python3.7/dist-packages (0.5.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Copying files: 0 files [00:00, ? files/s]\u001b[A\n",
            "Copying files: 1 files [00:00,  3.44 files/s]\u001b[A\n",
            "Copying files: 9 files [00:00, 13.09 files/s]\u001b[A\n",
            "Copying files: 19 files [00:00, 26.90 files/s]\u001b[A\n",
            "Copying files: 27 files [00:00, 36.48 files/s]\u001b[A\n",
            "Copying files: 34 files [00:01, 42.64 files/s]\u001b[A\n",
            "Copying files: 40 files [00:01, 46.12 files/s]\u001b[A\n",
            "Copying files: 52 files [00:01, 62.73 files/s]\u001b[A\n",
            "Copying files: 60 files [00:01, 62.03 files/s]\u001b[A\n",
            "Copying files: 70 files [00:01, 67.95 files/s]\u001b[A\n",
            "Copying files: 80 files [00:01, 75.03 files/s]\u001b[A\n",
            "Copying files: 89 files [00:01, 75.88 files/s]\u001b[A\n",
            "Copying files: 99 files [00:01, 81.64 files/s]\u001b[A\n",
            "Copying files: 108 files [00:02, 80.66 files/s]\u001b[A\n",
            "Copying files: 118 files [00:02, 85.43 files/s]\u001b[A\n",
            "Copying files: 127 files [00:02, 83.16 files/s]\u001b[A\n",
            "Copying files: 137 files [00:02, 86.54 files/s]\u001b[A\n",
            "Copying files: 146 files [00:02, 84.95 files/s]\u001b[A\n",
            "Copying files: 156 files [00:02, 86.20 files/s]\u001b[A\n",
            "Copying files: 166 files [00:02, 89.42 files/s]\u001b[A\n",
            "Copying files: 176 files [00:02, 86.09 files/s]\u001b[A\n",
            "Copying files: 186 files [00:02, 89.33 files/s]\u001b[A\n",
            "Copying files: 196 files [00:03, 79.12 files/s]\u001b[A\n",
            "Copying files: 205 files [00:03, 70.99 files/s]\u001b[A\n",
            "Copying files: 213 files [00:03, 71.49 files/s]\u001b[A\n",
            "Copying files: 221 files [00:03, 65.59 files/s]\u001b[A\n",
            "Copying files: 229 files [00:03, 67.83 files/s]\u001b[A\n",
            "Copying files: 236 files [00:03, 66.64 files/s]\u001b[A\n",
            "Copying files: 243 files [00:03, 65.14 files/s]\u001b[A\n",
            "Copying files: 250 files [00:03, 60.98 files/s]\u001b[A\n",
            "Copying files: 257 files [00:04, 63.01 files/s]\u001b[A\n",
            "Copying files: 264 files [00:04, 63.09 files/s]\u001b[A\n",
            "Copying files: 272 files [00:04, 67.37 files/s]\u001b[A\n",
            "Copying files: 279 files [00:04, 65.83 files/s]\u001b[A\n",
            "Copying files: 286 files [00:04, 66.82 files/s]\u001b[A\n",
            "Copying files: 293 files [00:04, 66.49 files/s]\u001b[A\n",
            "Copying files: 301 files [00:04, 69.23 files/s]\u001b[A\n",
            "Copying files: 308 files [00:04, 67.60 files/s]\u001b[A\n",
            "Copying files: 316 files [00:04, 68.94 files/s]\u001b[A\n",
            "Copying files: 323 files [00:05, 62.26 files/s]\u001b[A\n",
            "Copying files: 330 files [00:05, 57.95 files/s]\u001b[A\n",
            "Copying files: 337 files [00:05, 60.98 files/s]\u001b[A\n",
            "Copying files: 344 files [00:05, 60.93 files/s]\u001b[A\n",
            "Copying files: 352 files [00:05, 64.91 files/s]\u001b[A\n",
            "Copying files: 359 files [00:05, 59.53 files/s]\u001b[A\n",
            "Copying files: 367 files [00:05, 61.17 files/s]\u001b[A\n",
            "Copying files: 374 files [00:05, 56.49 files/s]\u001b[A\n",
            "Copying files: 381 files [00:06, 59.11 files/s]\u001b[A\n",
            "Copying files: 388 files [00:06, 56.73 files/s]\u001b[A\n",
            "Copying files: 395 files [00:06, 57.95 files/s]\u001b[A\n",
            "Copying files: 401 files [00:06, 57.48 files/s]\u001b[A\n",
            "Copying files: 407 files [00:06, 54.44 files/s]\u001b[A\n",
            "Copying files: 413 files [00:06, 38.48 files/s]\u001b[A\n",
            "Copying files: 420 files [00:06, 43.99 files/s]\u001b[A\n",
            "Copying files: 426 files [00:07, 44.45 files/s]\u001b[A\n",
            "Copying files: 431 files [00:07, 41.50 files/s]\u001b[A\n",
            "Copying files: 439 files [00:07, 49.57 files/s]\u001b[A\n",
            "Copying files: 445 files [00:07, 51.02 files/s]\u001b[A\n",
            "Copying files: 452 files [00:07, 55.36 files/s]\u001b[A\n",
            "Copying files: 458 files [00:07, 55.23 files/s]\u001b[A\n",
            "Copying files: 464 files [00:07, 55.78 files/s]\u001b[A\n",
            "Copying files: 472 files [00:07, 61.60 files/s]\u001b[A\n",
            "Copying files: 479 files [00:07, 57.76 files/s]\u001b[A\n",
            "Copying files: 485 files [00:08, 58.20 files/s]\u001b[A\n",
            "Copying files: 491 files [00:08, 54.95 files/s]\u001b[A\n",
            "Copying files: 499 files [00:08, 60.56 files/s]\u001b[A\n",
            "Copying files: 506 files [00:08, 59.15 files/s]\u001b[A\n",
            "Copying files: 512 files [00:08, 52.71 files/s]\u001b[A\n",
            "Copying files: 519 files [00:08, 53.85 files/s]\u001b[A\n",
            "Copying files: 527 files [00:08, 59.72 files/s]\u001b[A\n",
            "Copying files: 535 files [00:08, 61.72 files/s]\u001b[A\n",
            "Copying files: 546 files [00:08, 72.58 files/s]\u001b[A\n",
            "Copying files: 554 files [00:09, 74.23 files/s]\u001b[A\n",
            "Copying files: 564 files [00:09, 79.56 files/s]\u001b[A\n",
            "Copying files: 573 files [00:09, 79.23 files/s]\u001b[A\n",
            "Copying files: 582 files [00:09, 80.64 files/s]\u001b[A\n",
            "Copying files: 592 files [00:09, 84.17 files/s]\u001b[A\n",
            "Copying files: 601 files [00:09, 82.48 files/s]\u001b[A\n",
            "Copying files: 610 files [00:09, 84.15 files/s]\u001b[A\n",
            "Copying files: 619 files [00:09, 80.34 files/s]\u001b[A\n",
            "Copying files: 629 files [00:09, 85.25 files/s]\u001b[A\n",
            "Copying files: 638 files [00:10, 85.07 files/s]\u001b[A\n",
            "Copying files: 648 files [00:10, 84.82 files/s]\u001b[A\n",
            "Copying files: 659 files [00:10, 89.01 files/s]\u001b[A\n",
            "Copying files: 668 files [00:10, 86.26 files/s]\u001b[A\n",
            "Copying files: 679 files [00:10, 91.20 files/s]\u001b[A\n",
            "Copying files: 689 files [00:10, 88.76 files/s]\u001b[A\n",
            "Copying files: 699 files [00:10, 91.19 files/s]\u001b[A\n",
            "Copying files: 709 files [00:10, 87.10 files/s]\u001b[A\n",
            "Copying files: 718 files [00:10, 87.17 files/s]\u001b[A\n",
            "Copying files: 727 files [00:11, 87.89 files/s]\u001b[A\n",
            "Copying files: 736 files [00:11, 85.57 files/s]\u001b[A\n",
            "Copying files: 746 files [00:11, 87.79 files/s]\u001b[A\n",
            "Copying files: 755 files [00:11, 84.11 files/s]\u001b[A\n",
            "Copying files: 765 files [00:11, 42.30 files/s]\u001b[A\n",
            "Copying files: 773 files [00:12, 48.26 files/s]\u001b[A\n",
            "Copying files: 780 files [00:12, 26.68 files/s]\u001b[A\n",
            "Copying files: 788 files [00:12, 32.88 files/s]\u001b[A\n",
            "Copying files: 799 files [00:12, 43.51 files/s]\u001b[A\n",
            "Copying files: 807 files [00:12, 49.54 files/s]\u001b[A\n",
            "Copying files: 816 files [00:13, 56.44 files/s]\u001b[A\n",
            "Copying files: 824 files [00:13, 61.47 files/s]\u001b[A\n",
            "Copying files: 834 files [00:13, 70.18 files/s]\u001b[A\n",
            "Copying files: 843 files [00:13, 73.52 files/s]\u001b[A\n",
            "Copying files: 853 files [00:13, 75.03 files/s]\u001b[A\n",
            "Copying files: 863 files [00:13, 80.66 files/s]\u001b[A\n",
            "Copying files: 872 files [00:13, 78.95 files/s]\u001b[A\n",
            "Copying files: 882 files [00:13, 82.19 files/s]\u001b[A\n",
            "Copying files: 891 files [00:13, 81.26 files/s]\u001b[A\n",
            "Copying files: 901 files [00:14, 81.14 files/s]\u001b[A\n",
            "Copying files: 911 files [00:14, 84.61 files/s]\u001b[A\n",
            "Copying files: 920 files [00:14, 82.17 files/s]\u001b[A\n",
            "Copying files: 930 files [00:14, 85.88 files/s]\u001b[A\n",
            "Copying files: 939 files [00:14, 83.27 files/s]\u001b[A\n",
            "Copying files: 949 files [00:14, 86.79 files/s]\u001b[A\n",
            "Copying files: 958 files [00:14, 84.37 files/s]\u001b[A\n",
            "Copying files: 967 files [00:14, 83.55 files/s]\u001b[A\n",
            "Copying files: 976 files [00:14, 82.58 files/s]\u001b[A\n",
            "Copying files: 985 files [00:15, 81.09 files/s]\u001b[A\n",
            "Copying files: 995 files [00:15, 84.58 files/s]\u001b[A\n",
            "Copying files: 1004 files [00:15, 82.96 files/s]\u001b[A\n",
            "Copying files: 1013 files [00:15, 80.55 files/s]\u001b[A\n",
            "Copying files: 1023 files [00:15, 84.31 files/s]\u001b[A\n",
            "Copying files: 1032 files [00:15, 82.38 files/s]\u001b[A\n",
            "Copying files: 1041 files [00:15, 81.93 files/s]\u001b[A\n",
            "Copying files: 1051 files [00:15, 85.52 files/s]\u001b[A\n",
            "Copying files: 1060 files [00:15, 83.07 files/s]\u001b[A\n",
            "Copying files: 1070 files [00:16, 86.08 files/s]\u001b[A\n",
            "Copying files: 1079 files [00:16, 83.49 files/s]\u001b[A\n",
            "Copying files: 1088 files [00:16, 82.88 files/s]\u001b[A\n",
            "Copying files: 1097 files [00:16, 81.28 files/s]\u001b[A\n",
            "Copying files: 1106 files [00:16, 80.07 files/s]\u001b[A\n",
            "Copying files: 1116 files [00:16, 84.40 files/s]\u001b[A\n",
            "Copying files: 1125 files [00:16, 82.36 files/s]\u001b[A\n",
            "Copying files: 1134 files [00:16, 81.53 files/s]\u001b[A\n",
            "Copying files: 1143 files [00:16, 83.40 files/s]\u001b[A\n",
            "Copying files: 1152 files [00:17, 83.31 files/s]\u001b[A\n",
            "Copying files: 1162 files [00:17, 85.60 files/s]\u001b[A\n",
            "Copying files: 1171 files [00:17, 82.12 files/s]\u001b[A\n",
            "Copying files: 1180 files [00:17, 80.01 files/s]\u001b[A\n",
            "Copying files: 1191 files [00:17, 86.19 files/s]\u001b[A\n",
            "Copying files: 1200 files [00:17, 83.13 files/s]\u001b[A\n",
            "Copying files: 1210 files [00:17, 81.71 files/s]\u001b[A\n",
            "Copying files: 1220 files [00:17, 85.37 files/s]\u001b[A\n",
            "Copying files: 1229 files [00:18, 79.08 files/s]\u001b[A\n",
            "Copying files: 1239 files [00:18, 83.63 files/s]\u001b[A\n",
            "Copying files: 1248 files [00:18, 82.43 files/s]\u001b[A\n",
            "Copying files: 1257 files [00:18, 79.81 files/s]\u001b[A\n",
            "Copying files: 1266 files [00:18, 82.41 files/s]\u001b[A\n",
            "Copying files: 1275 files [00:18, 81.62 files/s]\u001b[A\n",
            "Copying files: 1284 files [00:18, 80.32 files/s]\u001b[A\n",
            "Copying files: 1294 files [00:18, 84.75 files/s]\u001b[A\n",
            "Copying files: 1303 files [00:18, 83.18 files/s]\u001b[A\n",
            "Copying files: 1312 files [00:19, 81.61 files/s]\u001b[A\n",
            "Copying files: 1322 files [00:19, 86.39 files/s]\u001b[A\n",
            "Copying files: 1331 files [00:19, 83.19 files/s]\u001b[A\n",
            "Copying files: 1340 files [00:19, 81.37 files/s]\u001b[A\n",
            "Copying files: 1351 files [00:19, 87.17 files/s]\u001b[A\n",
            "Copying files: 1360 files [00:19, 85.96 files/s]\u001b[A\n",
            "Copying files: 1369 files [00:19, 84.52 files/s]\u001b[A\n",
            "Copying files: 1380 files [00:19, 89.34 files/s]\u001b[A\n",
            "Copying files: 1389 files [00:19, 85.07 files/s]\u001b[A\n",
            "Copying files: 1398 files [00:20, 82.19 files/s]\u001b[A\n",
            "Copying files: 1407 files [00:20, 83.72 files/s]\u001b[A\n",
            "Copying files: 1416 files [00:20, 81.55 files/s]\u001b[A\n",
            "Copying files: 1425 files [00:20, 79.37 files/s]\u001b[A\n",
            "Copying files: 1435 files [00:20, 84.08 files/s]\u001b[A\n",
            "Copying files: 1444 files [00:20, 82.46 files/s]\u001b[A\n",
            "Copying files: 1454 files [00:20, 83.05 files/s]\u001b[A\n",
            "Copying files: 1464 files [00:20, 86.53 files/s]\u001b[A\n",
            "Copying files: 1473 files [00:20, 84.53 files/s]\u001b[A\n",
            "Copying files: 1482 files [00:21, 81.04 files/s]\u001b[A\n",
            "Copying files: 1491 files [00:21, 82.91 files/s]\u001b[A\n",
            "Copying files: 1500 files [00:21, 80.07 files/s]\u001b[A\n",
            "Copying files: 1509 files [00:21, 81.60 files/s]\u001b[A\n",
            "Copying files: 1518 files [00:21, 76.20 files/s]\u001b[A\n",
            "Copying files: 1526 files [00:21, 75.06 files/s]\u001b[A\n",
            "Copying files: 1536 files [00:21, 80.80 files/s]\u001b[A\n",
            "Copying files: 1545 files [00:21, 80.15 files/s]\u001b[A\n",
            "Copying files: 1554 files [00:21, 79.18 files/s]\u001b[A\n",
            "Copying files: 1563 files [00:22, 81.69 files/s]\u001b[A\n",
            "Copying files: 1572 files [00:22, 79.26 files/s]\u001b[A\n",
            "Copying files: 1581 files [00:22, 80.41 files/s]\u001b[A\n",
            "Copying files: 1590 files [00:22, 81.69 files/s]\u001b[A\n",
            "Copying files: 1599 files [00:22, 81.30 files/s]\u001b[A\n",
            "Copying files: 1609 files [00:22, 80.44 files/s]\u001b[A\n",
            "Copying files: 1619 files [00:22, 83.94 files/s]\u001b[A\n",
            "Copying files: 1628 files [00:22, 78.31 files/s]\u001b[A\n",
            "Copying files: 1637 files [00:23, 76.99 files/s]\u001b[A\n",
            "Copying files: 1646 files [00:23, 79.51 files/s]\u001b[A\n",
            "Copying files: 1655 files [00:23, 79.34 files/s]\u001b[A\n",
            "Copying files: 1665 files [00:23, 83.64 files/s]\u001b[A\n",
            "Copying files: 1674 files [00:23, 82.80 files/s]\u001b[A\n",
            "Copying files: 1684 files [00:23, 86.86 files/s]\u001b[A\n",
            "Copying files: 1693 files [00:23, 84.33 files/s]\u001b[A\n",
            "Copying files: 1703 files [00:23, 84.01 files/s]\u001b[A\n",
            "Copying files: 1714 files [00:23, 90.69 files/s]\u001b[A\n",
            "Copying files: 1724 files [00:24, 86.48 files/s]\u001b[A\n",
            "Copying files: 1734 files [00:24, 89.65 files/s]\u001b[A\n",
            "Copying files: 1744 files [00:24, 85.91 files/s]\u001b[A\n",
            "Copying files: 1753 files [00:24, 81.72 files/s]\u001b[A\n",
            "Copying files: 1762 files [00:24, 82.92 files/s]\u001b[A\n",
            "Copying files: 1771 files [00:24, 82.87 files/s]\u001b[A\n",
            "Copying files: 1781 files [00:24, 87.00 files/s]\u001b[A\n",
            "Copying files: 1790 files [00:24, 84.64 files/s]\u001b[A\n",
            "Copying files: 1801 files [00:24, 90.58 files/s]\u001b[A\n",
            "Copying files: 1811 files [00:25, 88.09 files/s]\u001b[A\n",
            "Copying files: 1820 files [00:25, 83.73 files/s]\u001b[A\n",
            "Copying files: 1830 files [00:25, 86.78 files/s]\u001b[A\n",
            "Copying files: 1839 files [00:25, 86.44 files/s]\u001b[A\n",
            "Copying files: 1849 files [00:25, 88.42 files/s]\u001b[A\n",
            "Copying files: 1858 files [00:25, 86.40 files/s]\u001b[A\n",
            "Copying files: 1867 files [00:25, 48.03 files/s]\u001b[A\n",
            "Copying files: 1875 files [00:26, 31.60 files/s]\u001b[A\n",
            "Copying files: 1885 files [00:26, 40.48 files/s]\u001b[A\n",
            "Copying files: 1894 files [00:26, 47.65 files/s]\u001b[A\n",
            "Copying files: 1905 files [00:26, 56.00 files/s]\u001b[A\n",
            "Copying files: 1914 files [00:26, 61.83 files/s]\u001b[A\n",
            "Copying files: 1922 files [00:26, 64.61 files/s]\u001b[A\n",
            "Copying files: 1932 files [00:27, 72.85 files/s]\u001b[A\n",
            "Copying files: 1941 files [00:27, 72.20 files/s]\u001b[A\n",
            "Copying files: 1950 files [00:27, 75.80 files/s]\u001b[A\n",
            "Copying files: 1959 files [00:27, 39.40 files/s]\u001b[A\n",
            "Copying files: 1968 files [00:27, 46.85 files/s]\u001b[A\n",
            "Copying files: 1975 files [00:28, 50.33 files/s]\u001b[A\n",
            "Copying files: 1984 files [00:28, 57.71 files/s]\u001b[A\n",
            "Copying files: 1992 files [00:28, 60.98 files/s]\u001b[A\n",
            "Copying files: 2001 files [00:28, 67.69 files/s]\u001b[A\n",
            "Copying files: 2009 files [00:28, 68.94 files/s]\u001b[A\n",
            "Copying files: 2018 files [00:28, 72.88 files/s]\u001b[A\n",
            "Copying files: 2026 files [00:28, 71.37 files/s]\u001b[A\n",
            "Copying files: 2035 files [00:28, 76.08 files/s]\u001b[A\n",
            "Copying files: 2043 files [00:28, 74.75 files/s]\u001b[A\n",
            "Copying files: 2053 files [00:29, 74.32 files/s]\u001b[A\n",
            "Copying files: 2062 files [00:29, 77.03 files/s]\u001b[A\n",
            "Copying files: 2070 files [00:29, 75.69 files/s]\u001b[A\n",
            "Copying files: 2080 files [00:29, 80.71 files/s]\u001b[A\n",
            "Copying files: 2089 files [00:29, 79.01 files/s]\u001b[A\n",
            "Copying files: 2097 files [00:29, 77.45 files/s]\u001b[A\n",
            "Copying files: 2105 files [00:29, 74.88 files/s]\u001b[A\n",
            "Copying files: 2114 files [00:29, 78.01 files/s]\u001b[A\n",
            "Copying files: 2122 files [00:29, 75.06 files/s]\u001b[A\n",
            "Copying files: 2131 files [00:30, 77.86 files/s]\u001b[A\n",
            "Copying files: 2139 files [00:30, 74.12 files/s]\u001b[A\n",
            "Copying files: 2147 files [00:30, 67.75 files/s]\u001b[A\n",
            "Copying files: 2155 files [00:30, 70.25 files/s]\u001b[A\n",
            "Copying files: 2163 files [00:30, 68.26 files/s]\u001b[A\n",
            "Copying files: 2172 files [00:30, 71.90 files/s]\u001b[A\n",
            "Copying files: 2180 files [00:30, 70.96 files/s]\u001b[A\n",
            "Copying files: 2189 files [00:30, 74.17 files/s]\u001b[A\n",
            "Copying files: 2197 files [00:30, 72.50 files/s]\u001b[A\n",
            "Copying files: 2206 files [00:31, 75.44 files/s]\u001b[A\n",
            "Copying files: 2214 files [00:31, 72.83 files/s]\u001b[A\n",
            "Copying files: 2222 files [00:31, 74.71 files/s]\u001b[A\n",
            "Copying files: 2230 files [00:31, 71.82 files/s]\u001b[A\n",
            "Copying files: 2239 files [00:31, 71.80 files/s]\u001b[A\n",
            "Copying files: 2247 files [00:31, 72.85 files/s]\u001b[A\n",
            "Copying files: 2255 files [00:31, 70.61 files/s]\u001b[A\n",
            "Copying files: 2263 files [00:31, 72.70 files/s]\u001b[A\n",
            "Copying files: 2271 files [00:32, 71.00 files/s]\u001b[A\n",
            "Copying files: 2280 files [00:32, 74.78 files/s]\u001b[A\n",
            "Copying files: 2288 files [00:32, 74.77 files/s]\u001b[A\n",
            "Copying files: 2298 files [00:32, 81.35 files/s]\u001b[A\n",
            "Copying files: 2307 files [00:32, 80.03 files/s]\u001b[A\n",
            "Copying files: 2316 files [00:32, 76.01 files/s]\u001b[A\n",
            "Copying files: 2326 files [00:32, 80.98 files/s]\u001b[A\n",
            "Copying files: 2335 files [00:32, 79.36 files/s]\u001b[A\n",
            "Copying files: 2345 files [00:32, 80.05 files/s]\u001b[A\n",
            "Copying files: 2355 files [00:33, 84.11 files/s]\u001b[A\n",
            "Copying files: 2364 files [00:33, 83.27 files/s]\u001b[A\n",
            "Copying files: 2374 files [00:33, 83.24 files/s]\u001b[A\n",
            "Copying files: 2385 files [00:33, 88.73 files/s]\u001b[A\n",
            "Copying files: 2394 files [00:33, 85.70 files/s]\u001b[A\n",
            "Copying files: 2403 files [00:33, 75.32 files/s]\u001b[A\n",
            "Copying files: 2411 files [00:33, 72.70 files/s]\u001b[A\n",
            "Copying files: 2419 files [00:33, 70.96 files/s]\u001b[A\n",
            "Copying files: 2429 files [00:33, 77.74 files/s]\u001b[A\n",
            "Copying files: 2437 files [00:34, 76.81 files/s]\u001b[A\n",
            "Copying files: 2445 files [00:34, 75.38 files/s]\u001b[A\n",
            "Copying files: 2455 files [00:34, 80.69 files/s]\u001b[A\n",
            "Copying files: 2464 files [00:34, 66.72 files/s]\u001b[A\n",
            "Copying files: 2473 files [00:34, 70.43 files/s]\u001b[A\n",
            "Copying files: 2483 files [00:34, 76.88 files/s]\u001b[A\n",
            "Copying files: 2492 files [00:34, 77.09 files/s]\u001b[A\n",
            "Copying files: 2502 files [00:34, 81.76 files/s]\u001b[A\n",
            "Copying files: 2511 files [00:35, 81.82 files/s]\u001b[A\n",
            "Copying files: 2520 files [00:35, 75.87 files/s]\u001b[A\n",
            "Copying files: 2528 files [00:35, 73.69 files/s]\u001b[A\n",
            "Copying files: 2538 files [00:35, 76.57 files/s]\u001b[A\n",
            "Copying files: 2549 files [00:35, 83.33 files/s]\u001b[A\n",
            "Copying files: 2558 files [00:35, 83.32 files/s]\u001b[A\n",
            "Copying files: 2568 files [00:35, 86.86 files/s]\u001b[A\n",
            "Copying files: 2577 files [00:35, 84.73 files/s]\u001b[A\n",
            "Copying files: 2587 files [00:35, 88.58 files/s]\u001b[A\n",
            "Copying files: 2596 files [00:36, 86.72 files/s]\u001b[A\n",
            "Copying files: 2606 files [00:36, 90.24 files/s]\u001b[A\n",
            "Copying files: 2616 files [00:36, 88.74 files/s]\u001b[A\n",
            "Copying files: 2625 files [00:36, 85.68 files/s]\u001b[A\n",
            "Copying files: 2634 files [00:36, 84.62 files/s]\u001b[A\n",
            "Copying files: 2643 files [00:36, 84.05 files/s]\u001b[A\n",
            "Copying files: 2652 files [00:36, 50.14 files/s]\u001b[A\n",
            "Copying files: 2659 files [00:37, 53.19 files/s]\u001b[A\n",
            "Copying files: 2666 files [00:37, 43.13 files/s]\u001b[A\n",
            "Copying files: 2672 files [00:37, 40.17 files/s]\u001b[A\n",
            "Copying files: 2680 files [00:37, 47.25 files/s]\u001b[A\n",
            "Copying files: 2690 files [00:37, 57.26 files/s]\u001b[A\n",
            "Copying files: 2698 files [00:37, 62.15 files/s]\u001b[A\n",
            "Copying files: 2708 files [00:37, 70.12 files/s]\u001b[A\n",
            "Copying files: 2716 files [00:38, 72.14 files/s]\u001b[A\n",
            "Copying files: 2726 files [00:38, 74.66 files/s]\u001b[A\n",
            "Copying files: 2737 files [00:38, 81.66 files/s]\u001b[A\n",
            "Copying files: 2746 files [00:38, 79.02 files/s]\u001b[A\n",
            "Copying files: 2756 files [00:38, 83.83 files/s]\u001b[A\n",
            "Copying files: 2765 files [00:38, 82.91 files/s]\u001b[A\n",
            "Copying files: 2775 files [00:38, 86.45 files/s]\u001b[A\n",
            "Copying files: 2784 files [00:38, 79.91 files/s]\u001b[A\n",
            "Copying files: 2793 files [00:38, 80.86 files/s]\u001b[A\n",
            "Copying files: 2803 files [00:39, 85.38 files/s]\u001b[A\n",
            "Copying files: 2812 files [00:39, 81.21 files/s]\u001b[A\n",
            "Copying files: 2822 files [00:39, 85.48 files/s]\u001b[A\n",
            "Copying files: 2831 files [00:39, 82.97 files/s]\u001b[A\n",
            "Copying files: 2841 files [00:39, 85.48 files/s]\u001b[A\n",
            "Copying files: 2850 files [00:39, 84.57 files/s]\u001b[A\n",
            "Copying files: 2859 files [00:39, 82.95 files/s]\u001b[A\n",
            "Copying files: 2868 files [00:39, 82.13 files/s]\u001b[A\n",
            "Copying files: 2877 files [00:39, 80.82 files/s]\u001b[A\n",
            "Copying files: 2887 files [00:40, 84.66 files/s]\u001b[A\n",
            "Copying files: 2896 files [00:40, 83.54 files/s]\u001b[A\n",
            "Copying files: 2905 files [00:40, 81.89 files/s]\u001b[A\n",
            "Copying files: 2915 files [00:40, 84.59 files/s]\u001b[A\n",
            "Copying files: 2924 files [00:40, 83.17 files/s]\u001b[A\n",
            "Copying files: 2934 files [00:40, 86.47 files/s]\u001b[A\n",
            "Copying files: 2943 files [00:40, 84.63 files/s]\u001b[A\n",
            "Copying files: 2952 files [00:40, 82.21 files/s]\u001b[A\n",
            "Copying files: 2963 files [00:40, 88.90 files/s]\u001b[A\n",
            "Copying files: 2972 files [00:41, 84.50 files/s]\u001b[A\n",
            "Copying files: 2982 files [00:41, 84.19 files/s]\u001b[A\n",
            "Copying files: 2992 files [00:41, 87.87 files/s]\u001b[A\n",
            "Copying files: 3001 files [00:41, 84.88 files/s]\u001b[A\n",
            "Copying files: 3012 files [00:41, 90.26 files/s]\u001b[A\n",
            "Copying files: 3022 files [00:41, 88.44 files/s]\u001b[A\n",
            "Copying files: 3031 files [00:41, 85.80 files/s]\u001b[A\n",
            "Copying files: 3041 files [00:41, 88.42 files/s]\u001b[A\n",
            "Copying files: 3050 files [00:41, 85.30 files/s]\u001b[A\n",
            "Copying files: 3059 files [00:42, 82.58 files/s]\u001b[A\n",
            "Copying files: 3070 files [00:42, 87.95 files/s]\u001b[A\n",
            "Copying files: 3079 files [00:42, 81.47 files/s]\u001b[A\n",
            "Copying files: 3089 files [00:42, 81.22 files/s]\u001b[A\n",
            "Copying files: 3099 files [00:42, 85.24 files/s]\u001b[A\n",
            "Copying files: 3108 files [00:42, 82.61 files/s]\u001b[A\n",
            "Copying files: 3118 files [00:42, 86.12 files/s]\u001b[A\n",
            "Copying files: 3127 files [00:42, 82.88 files/s]\u001b[A\n",
            "Copying files: 3136 files [00:43, 80.71 files/s]\u001b[A\n",
            "Copying files: 3146 files [00:43, 85.00 files/s]\u001b[A\n",
            "Copying files: 3155 files [00:43, 82.61 files/s]\u001b[A\n",
            "Copying files: 3164 files [00:43, 82.55 files/s]\u001b[A\n",
            "Copying files: 3174 files [00:43, 85.16 files/s]\u001b[A\n",
            "Copying files: 3183 files [00:43, 83.54 files/s]\u001b[A\n",
            "Copying files: 3192 files [00:43, 81.06 files/s]\u001b[A\n",
            "Copying files: 3202 files [00:43, 84.31 files/s]\u001b[A\n",
            "Copying files: 3211 files [00:43, 80.64 files/s]\u001b[A\n",
            "Copying files: 3220 files [00:44, 78.40 files/s]\u001b[A\n",
            "Copying files: 3230 files [00:44, 82.91 files/s]\u001b[A\n",
            "Copying files: 3239 files [00:44, 81.57 files/s]\u001b[A\n",
            "Copying files: 3249 files [00:44, 85.10 files/s]\u001b[A\n",
            "Copying files: 3258 files [00:44, 83.87 files/s]\u001b[A\n",
            "Copying files: 3268 files [00:44, 87.80 files/s]\u001b[A\n",
            "Copying files: 3277 files [00:44, 85.17 files/s]\u001b[A\n",
            "Copying files: 3286 files [00:44, 83.28 files/s]\u001b[A\n",
            "Copying files: 3296 files [00:44, 86.55 files/s]\u001b[A\n",
            "Copying files: 3305 files [00:45, 84.05 files/s]\u001b[A\n",
            "Copying files: 3315 files [00:45, 86.56 files/s]\u001b[A\n",
            "Copying files: 3324 files [00:45, 84.50 files/s]\u001b[A\n",
            "Copying files: 3334 files [00:45, 87.99 files/s]\u001b[A\n",
            "Copying files: 3343 files [00:45, 83.75 files/s]\u001b[A\n",
            "Copying files: 3353 files [00:45, 87.42 files/s]\u001b[A\n",
            "Copying files: 3362 files [00:45, 84.24 files/s]\u001b[A\n",
            "Copying files: 3371 files [00:45, 82.53 files/s]\u001b[A\n",
            "Copying files: 3381 files [00:45, 85.61 files/s]\u001b[A\n",
            "Copying files: 3390 files [00:46, 42.85 files/s]\u001b[A\n",
            "Copying files: 3397 files [00:46, 43.88 files/s]\u001b[A\n",
            "Copying files: 3404 files [00:46, 38.25 files/s]\u001b[A\n",
            "Copying files: 3410 files [00:47, 33.41 files/s]\u001b[A\n",
            "Copying files: 3420 files [00:47, 43.64 files/s]\u001b[A\n",
            "Copying files: 3426 files [00:47, 46.66 files/s]\u001b[A\n",
            "Copying files: 3436 files [00:47, 57.29 files/s]\u001b[A\n",
            "Copying files: 3445 files [00:47, 63.44 files/s]\u001b[A\n",
            "Copying files: 3455 files [00:47, 69.21 files/s]\u001b[A\n",
            "Copying files: 3465 files [00:47, 75.42 files/s]\u001b[A\n",
            "Copying files: 3474 files [00:48, 37.68 files/s]\u001b[A\n",
            "Copying files: 3484 files [00:48, 46.54 files/s]\u001b[A\n",
            "Copying files: 3492 files [00:48, 51.28 files/s]\u001b[A\n",
            "Copying files: 3502 files [00:48, 60.21 files/s]\u001b[A\n",
            "Copying files: 3510 files [00:48, 63.90 files/s]\u001b[A\n",
            "Copying files: 3520 files [00:48, 72.23 files/s]\u001b[A\n",
            "Copying files: 3529 files [00:48, 73.36 files/s]\u001b[A\n",
            "Copying files: 3538 files [00:48, 74.35 files/s]\u001b[A\n",
            "Copying files: 3548 files [00:49, 80.20 files/s]\u001b[A\n",
            "Copying files: 3557 files [00:49, 81.02 files/s]\u001b[A\n",
            "Copying files: 3567 files [00:49, 85.45 files/s]\u001b[A\n",
            "Copying files: 3576 files [00:49, 84.06 files/s]\u001b[A\n",
            "Copying files: 3585 files [00:49, 82.61 files/s]\u001b[A\n",
            "Copying files: 3596 files [00:49, 88.48 files/s]\u001b[A\n",
            "Copying files: 3605 files [00:49, 85.68 files/s]\u001b[A\n",
            "Copying files: 3615 files [00:49, 89.14 files/s]\u001b[A\n",
            "Copying files: 3624 files [00:49, 83.62 files/s]\u001b[A\n",
            "Copying files: 3633 files [00:50, 81.34 files/s]\u001b[A\n",
            "Copying files: 3642 files [00:50, 83.54 files/s]\u001b[A\n",
            "Copying files: 3651 files [00:50, 81.79 files/s]\u001b[A\n",
            "Copying files: 3661 files [00:50, 85.12 files/s]\u001b[A\n",
            "Copying files: 3670 files [00:50, 82.53 files/s]\u001b[A\n",
            "Copying files: 3679 files [00:50, 81.40 files/s]\u001b[A\n",
            "Copying files: 3689 files [00:50, 86.20 files/s]\u001b[A\n",
            "Copying files: 3698 files [00:50, 83.10 files/s]\u001b[A\n",
            "Copying files: 3708 files [00:50, 83.00 files/s]\u001b[A\n",
            "Copying files: 3718 files [00:51, 85.97 files/s]\u001b[A\n",
            "Copying files: 3727 files [00:51, 85.00 files/s]\u001b[A\n",
            "Copying files: 3736 files [00:51, 85.92 files/s]\u001b[A\n",
            "Copying files: 3745 files [00:51, 82.68 files/s]\u001b[A\n",
            "Copying files: 3754 files [00:51, 82.35 files/s]\u001b[A\n",
            "Copying files: 3764 files [00:51, 85.58 files/s]\u001b[A\n",
            "Copying files: 3773 files [00:51, 83.89 files/s]\u001b[A\n",
            "Copying files: 3783 files [00:51, 86.92 files/s]\u001b[A\n",
            "Copying files: 3792 files [00:51, 79.58 files/s]\u001b[A\n",
            "Copying files: 3801 files [00:52, 78.29 files/s]\u001b[A\n",
            "Copying files: 3811 files [00:52, 84.00 files/s]\u001b[A\n",
            "Copying files: 3820 files [00:52, 82.94 files/s]\u001b[A\n",
            "Copying files: 3830 files [00:52, 87.29 files/s]\u001b[A\n",
            "Copying files: 3839 files [00:52, 83.06 files/s]\u001b[A\n",
            "Copying files: 3848 files [00:52, 80.31 files/s]\u001b[A\n",
            "Copying files: 3858 files [00:52, 84.02 files/s]\u001b[A\n",
            "Copying files: 3867 files [00:52, 81.20 files/s]\u001b[A\n",
            "Copying files: 3877 files [00:53, 80.52 files/s]\u001b[A\n",
            "Copying files: 3887 files [00:53, 85.38 files/s]\u001b[A\n",
            "Copying files: 3896 files [00:53, 83.26 files/s]\u001b[A\n",
            "Copying files: 3905 files [00:53, 78.64 files/s]\u001b[A\n",
            "Copying files: 3915 files [00:53, 84.20 files/s]\u001b[A\n",
            "Copying files: 3924 files [00:53, 82.78 files/s]\u001b[A\n",
            "Copying files: 3933 files [00:53, 80.72 files/s]\u001b[A\n",
            "Copying files: 3943 files [00:53, 83.62 files/s]\u001b[A\n",
            "Copying files: 3952 files [00:53, 81.64 files/s]\u001b[A\n",
            "Copying files: 3961 files [00:54, 82.80 files/s]\u001b[A\n",
            "Copying files: 3972 files [00:54, 89.24 files/s]\u001b[A\n",
            "Copying files: 3981 files [00:54, 84.07 files/s]\u001b[A\n",
            "Copying files: 3990 files [00:54, 77.64 files/s]\u001b[A\n",
            "Copying files: 4000 files [00:54, 82.51 files/s]\u001b[A\n",
            "Copying files: 4009 files [00:54, 81.44 files/s]\u001b[A\n",
            "Copying files: 4018 files [00:54, 82.30 files/s]\u001b[A\n",
            "Copying files: 4028 files [00:54, 85.43 files/s]\u001b[A\n",
            "Copying files: 4037 files [00:54, 84.38 files/s]\u001b[A\n",
            "Copying files: 4046 files [00:55, 82.54 files/s]\u001b[A\n",
            "Copying files: 4057 files [00:55, 88.09 files/s]\u001b[A\n",
            "Copying files: 4066 files [00:55, 85.32 files/s]\u001b[A\n",
            "Copying files: 4075 files [00:55, 83.61 files/s]\u001b[A\n",
            "Copying files: 4085 files [00:55, 87.40 files/s]\u001b[A\n",
            "Copying files: 4094 files [00:55, 84.52 files/s]\u001b[A\n",
            "Copying files: 4103 files [00:55, 83.35 files/s]\u001b[A\n",
            "Copying files: 4113 files [00:55, 86.98 files/s]\u001b[A\n",
            "Copying files: 4122 files [00:55, 83.61 files/s]\u001b[A\n",
            "Copying files: 4131 files [00:56, 81.81 files/s]\u001b[A\n",
            "Copying files: 4141 files [00:56, 86.19 files/s]\u001b[A\n",
            "Copying files: 4150 files [00:56, 84.74 files/s]\u001b[A\n",
            "Copying files: 4159 files [00:56, 83.42 files/s]\u001b[A\n",
            "Copying files: 4169 files [00:56, 85.82 files/s]\u001b[A\n",
            "Copying files: 4178 files [00:56, 84.67 files/s]\u001b[A\n",
            "Copying files: 4189 files [00:56, 90.65 files/s]\u001b[A\n",
            "Copying files: 4199 files [00:56, 89.04 files/s]\u001b[A\n",
            "Copying files: 4208 files [00:56, 87.86 files/s]\u001b[A\n",
            "Copying files: 4217 files [00:57, 88.42 files/s]\u001b[A\n",
            "Copying files: 4226 files [00:57, 83.53 files/s]\u001b[A\n",
            "Copying files: 4236 files [00:57, 86.97 files/s]\u001b[A\n",
            "Copying files: 4245 files [00:57, 85.08 files/s]\u001b[A\n",
            "Copying files: 4256 files [00:57, 89.94 files/s]\u001b[A\n",
            "Copying files: 4266 files [00:57, 82.71 files/s]\u001b[A\n",
            "Copying files: 4275 files [00:57, 81.61 files/s]\u001b[A\n",
            "Copying files: 4284 files [00:57, 81.11 files/s]\u001b[A\n",
            "Copying files: 4293 files [00:57, 76.36 files/s]\u001b[A\n",
            "Copying files: 4302 files [00:58, 77.21 files/s]\u001b[A\n",
            "Copying files: 4311 files [00:58, 75.35 files/s]\u001b[A\n",
            "Copying files: 4321 files [00:58, 80.81 files/s]\u001b[A\n",
            "Copying files: 4330 files [00:58, 78.62 files/s]\u001b[A\n",
            "Copying files: 4340 files [00:58, 83.70 files/s]\u001b[A\n",
            "Copying files: 4349 files [00:58, 76.87 files/s]\u001b[A\n",
            "Copying files: 4357 files [00:58, 77.44 files/s]\u001b[A\n",
            "Copying files: 4367 files [00:58, 74.10 files/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data\n",
        "# Suggestion: automates image look-up/download from the web\n",
        "dataset_train = FlowerData(pathlib.Path(data_splitted)/\"train\", transforms=transform, train=True)\n",
        "dataset_test = FlowerData(pathlib.Path(data_splitted)/\"test\", transforms=transform_test, train=False)\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)\n",
        "# dataset_train, dataset_test = train_test_split(df, test_size=0.2, random_state=25)"
      ],
      "metadata": {
        "id": "pDzDg43wLW6s"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "G4ZIJhf9u3LS"
      },
      "outputs": [],
      "source": [
        "# Import data\n",
        "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnza0JBEu4Mg"
      },
      "source": [
        "Set model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "SwuosvXaqPC_"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model and move to the GPU\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model_ft = coatnet_0()\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = nn.Linear(num_ftrs, 200) #change the 2nd arg according to the number of Labels\n",
        "#TODO: look into why changing this arg fixes the error\n",
        "model_ft.to(DEVICE)#move the model to GPU\n",
        "# Choose the simple and violent Adam optimizer to reduce the learning rate\n",
        "optimizer = optim.Adam(model_ft.parameters(), lr=modellr)\n",
        "cosine_schedule = optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer,T_max=20,eta_min=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import torch\n",
        "# import torch.optim as optim\n",
        "# import torch.nn.functional as F\n",
        "# from torch.autograd import Variable\n",
        "# from torchvision import datasets, transforms\n",
        "\n",
        "# def train(rank, args, model):\n",
        "#     torch.manual_seed(args.seed + rank)\n",
        "#     for param in model.parameters():\n",
        "#         # Break gradient sharing\n",
        "#         param.grad.data = param.grad.data.clone()\n",
        "\n",
        "#     train_loader = torch.utils.data.DataLoader(\n",
        "#         datasets.MNIST('../data', train=True, download=True,\n",
        "#                     transform=transforms.Compose([\n",
        "#                         transforms.ToTensor(),\n",
        "#                         transforms.Normalize((0.1307,), (0.3081,))\n",
        "#                     ])),\n",
        "#         batch_size=args.batch_size, shuffle=True, num_workers=1)\n",
        "#     test_loader = torch.utils.data.DataLoader(\n",
        "#         datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "#                         transforms.ToTensor(),\n",
        "#                         transforms.Normalize((0.1307,), (0.3081,))\n",
        "#                     ])),\n",
        "#         batch_size=args.batch_size, shuffle=True, num_workers=1)\n",
        "\n",
        "#     optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
        "#     for epoch in range(1, args.epochs + 1):\n",
        "#         train_epoch(epoch, args, model, train_loader, optimizer)\n",
        "#         test_epoch(epoch, args, model, test_loader)\n",
        "\n",
        "\n",
        "# def train_epoch(epoch, args, model, data_loader, optimizer):\n",
        "#     model.train()\n",
        "#     pid = os.getpid()\n",
        "#     samples_seen = 0\n",
        "#     for batch_idx, (data, target) in enumerate(data_loader):\n",
        "#         data, target = Variable(data), Variable(target)\n",
        "#         optimizer.zero_grad()\n",
        "#         output = model(data)\n",
        "#         loss = F.nll_loss(output, target)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         if batch_idx % args.log_interval == 0:\n",
        "#             print('{}\\tTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "#                 pid, epoch, batch_idx * len(data), len(data_loader.dataset),\n",
        "#                 100. * batch_idx / len(data_loader), loss.data[0]))\n",
        "\n",
        "\n",
        "# def test_epoch(epoch, args, model, data_loader):\n",
        "#     model.eval()\n",
        "#     test_loss = 0\n",
        "#     correct = 0\n",
        "#     for data, target in data_loader:\n",
        "#         data, target = Variable(data, volatile=True), Variable(target)\n",
        "#         output = model(data)\n",
        "#         test_loss += F.nll_loss(output, target).data[0]\n",
        "#         pred = output.data.max(1)[1] # get the index of the max log-probability\n",
        "#         correct += pred.eq(target.data).cpu().sum()\n",
        "\n",
        "#     test_loss = test_loss\n",
        "#     test_loss /= len(data_loader) # loss function already averages over batch size\n",
        "#     print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "#         test_loss, correct, len(data_loader.dataset),\n",
        "#         100. * correct / len(data_loader.dataset)))"
      ],
      "metadata": {
        "id": "5PTDtb3eSGr3"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for epoch in range(1, EPOCHS + 1):\n",
        "#     train(model_ft, DEVICE, train_loader, optimizer, epoch)\n",
        "#     cosine_schedule.step()\n",
        "#     val(model_ft, DEVICE, test_loader)"
      ],
      "metadata": {
        "id": "wE4uvxHoTKPu"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "7slfcWv9qV-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15fab15a-c8b8-4715-9bf9-9a2fe3ec18a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2444 153\n",
            "Train Epoch: 1 [160/2444 (7%)]\tLoss: 1.802946\n",
            "Train Epoch: 1 [320/2444 (13%)]\tLoss: 1.690281\n",
            "Train Epoch: 1 [480/2444 (20%)]\tLoss: 1.792991\n",
            "Train Epoch: 1 [640/2444 (26%)]\tLoss: 1.110464\n",
            "Train Epoch: 1 [800/2444 (33%)]\tLoss: 1.447356\n",
            "Train Epoch: 1 [960/2444 (39%)]\tLoss: 0.894349\n",
            "Train Epoch: 1 [1120/2444 (46%)]\tLoss: 1.393182\n",
            "Train Epoch: 1 [1280/2444 (52%)]\tLoss: 0.996071\n",
            "Train Epoch: 1 [1440/2444 (59%)]\tLoss: 1.187883\n",
            "Train Epoch: 1 [1600/2444 (65%)]\tLoss: 1.211130\n",
            "Train Epoch: 1 [1760/2444 (72%)]\tLoss: 1.646637\n",
            "Train Epoch: 1 [1920/2444 (78%)]\tLoss: 0.847263\n",
            "Train Epoch: 1 [2080/2444 (85%)]\tLoss: 0.919376\n",
            "Train Epoch: 1 [2240/2444 (92%)]\tLoss: 1.118716\n",
            "Train Epoch: 1 [2400/2444 (98%)]\tLoss: 1.226440\n",
            "epoch:1,loss:1.3574571679620182\n",
            "132 9\n",
            "\n",
            "Val set: Average loss: 0.9253, Accuracy: 86/132 (65%)\n",
            "\n",
            "2444 153\n",
            "Train Epoch: 2 [160/2444 (7%)]\tLoss: 0.560436\n",
            "Train Epoch: 2 [320/2444 (13%)]\tLoss: 0.832161\n",
            "Train Epoch: 2 [480/2444 (20%)]\tLoss: 1.030626\n",
            "Train Epoch: 2 [640/2444 (26%)]\tLoss: 0.747495\n",
            "Train Epoch: 2 [800/2444 (33%)]\tLoss: 0.826073\n",
            "Train Epoch: 2 [960/2444 (39%)]\tLoss: 0.791653\n",
            "Train Epoch: 2 [1120/2444 (46%)]\tLoss: 1.320427\n",
            "Train Epoch: 2 [1280/2444 (52%)]\tLoss: 0.443378\n",
            "Train Epoch: 2 [1440/2444 (59%)]\tLoss: 0.778128\n",
            "Train Epoch: 2 [1600/2444 (65%)]\tLoss: 1.035152\n",
            "Train Epoch: 2 [1760/2444 (72%)]\tLoss: 1.002752\n",
            "Train Epoch: 2 [1920/2444 (78%)]\tLoss: 1.142585\n",
            "Train Epoch: 2 [2080/2444 (85%)]\tLoss: 1.271154\n",
            "Train Epoch: 2 [2240/2444 (92%)]\tLoss: 1.018170\n",
            "Train Epoch: 2 [2400/2444 (98%)]\tLoss: 0.930629\n",
            "epoch:2,loss:1.000970752800212\n",
            "132 9\n",
            "\n",
            "Val set: Average loss: 0.7927, Accuracy: 96/132 (73%)\n",
            "\n",
            "2444 153\n",
            "Train Epoch: 3 [160/2444 (7%)]\tLoss: 0.353087\n",
            "Train Epoch: 3 [320/2444 (13%)]\tLoss: 0.359754\n",
            "Train Epoch: 3 [480/2444 (20%)]\tLoss: 0.721209\n",
            "Train Epoch: 3 [640/2444 (26%)]\tLoss: 0.666031\n",
            "Train Epoch: 3 [800/2444 (33%)]\tLoss: 1.217224\n",
            "Train Epoch: 3 [960/2444 (39%)]\tLoss: 1.397194\n",
            "Train Epoch: 3 [1120/2444 (46%)]\tLoss: 0.640942\n",
            "Train Epoch: 3 [1280/2444 (52%)]\tLoss: 0.474406\n",
            "Train Epoch: 3 [1440/2444 (59%)]\tLoss: 0.789385\n",
            "Train Epoch: 3 [1600/2444 (65%)]\tLoss: 0.945940\n",
            "Train Epoch: 3 [1760/2444 (72%)]\tLoss: 0.601804\n",
            "Train Epoch: 3 [1920/2444 (78%)]\tLoss: 0.907781\n",
            "Train Epoch: 3 [2080/2444 (85%)]\tLoss: 1.078068\n",
            "Train Epoch: 3 [2240/2444 (92%)]\tLoss: 0.847847\n",
            "Train Epoch: 3 [2400/2444 (98%)]\tLoss: 1.148636\n",
            "epoch:3,loss:0.7995039770026612\n",
            "132 9\n",
            "\n",
            "Val set: Average loss: 0.9551, Accuracy: 87/132 (66%)\n",
            "\n",
            "2444 153\n",
            "Train Epoch: 4 [160/2444 (7%)]\tLoss: 0.385564\n",
            "Train Epoch: 4 [320/2444 (13%)]\tLoss: 1.150644\n",
            "Train Epoch: 4 [480/2444 (20%)]\tLoss: 0.644199\n",
            "Train Epoch: 4 [640/2444 (26%)]\tLoss: 0.869206\n",
            "Train Epoch: 4 [800/2444 (33%)]\tLoss: 0.763997\n",
            "Train Epoch: 4 [960/2444 (39%)]\tLoss: 0.425067\n",
            "Train Epoch: 4 [1120/2444 (46%)]\tLoss: 0.600739\n",
            "Train Epoch: 4 [1280/2444 (52%)]\tLoss: 0.928850\n",
            "Train Epoch: 4 [1440/2444 (59%)]\tLoss: 0.444787\n",
            "Train Epoch: 4 [1600/2444 (65%)]\tLoss: 0.585421\n",
            "Train Epoch: 4 [1760/2444 (72%)]\tLoss: 0.623357\n",
            "Train Epoch: 4 [1920/2444 (78%)]\tLoss: 0.514438\n",
            "Train Epoch: 4 [2080/2444 (85%)]\tLoss: 1.207259\n",
            "Train Epoch: 4 [2240/2444 (92%)]\tLoss: 0.670731\n",
            "Train Epoch: 4 [2400/2444 (98%)]\tLoss: 0.697738\n",
            "epoch:4,loss:0.6904701966475817\n",
            "132 9\n",
            "\n",
            "Val set: Average loss: 0.7231, Accuracy: 99/132 (75%)\n",
            "\n",
            "2444 153\n",
            "Train Epoch: 5 [160/2444 (7%)]\tLoss: 0.568215\n",
            "Train Epoch: 5 [320/2444 (13%)]\tLoss: 0.542385\n",
            "Train Epoch: 5 [480/2444 (20%)]\tLoss: 0.175504\n",
            "Train Epoch: 5 [640/2444 (26%)]\tLoss: 0.869340\n",
            "Train Epoch: 5 [800/2444 (33%)]\tLoss: 0.590931\n",
            "Train Epoch: 5 [960/2444 (39%)]\tLoss: 0.495402\n",
            "Train Epoch: 5 [1120/2444 (46%)]\tLoss: 1.474320\n",
            "Train Epoch: 5 [1280/2444 (52%)]\tLoss: 1.160632\n",
            "Train Epoch: 5 [1440/2444 (59%)]\tLoss: 0.418446\n",
            "Train Epoch: 5 [1600/2444 (65%)]\tLoss: 0.501709\n",
            "Train Epoch: 5 [1760/2444 (72%)]\tLoss: 0.677943\n",
            "Train Epoch: 5 [1920/2444 (78%)]\tLoss: 0.714899\n",
            "Train Epoch: 5 [2080/2444 (85%)]\tLoss: 0.719991\n",
            "Train Epoch: 5 [2240/2444 (92%)]\tLoss: 0.723700\n",
            "Train Epoch: 5 [2400/2444 (98%)]\tLoss: 0.902307\n",
            "epoch:5,loss:0.6263939456807243\n",
            "132 9\n",
            "\n",
            "Val set: Average loss: 0.7869, Accuracy: 95/132 (72%)\n",
            "\n",
            "2444 153\n",
            "Train Epoch: 6 [160/2444 (7%)]\tLoss: 0.560103\n",
            "Train Epoch: 6 [320/2444 (13%)]\tLoss: 0.430179\n",
            "Train Epoch: 6 [480/2444 (20%)]\tLoss: 0.695261\n",
            "Train Epoch: 6 [640/2444 (26%)]\tLoss: 0.891946\n",
            "Train Epoch: 6 [800/2444 (33%)]\tLoss: 0.692344\n",
            "Train Epoch: 6 [960/2444 (39%)]\tLoss: 0.556804\n",
            "Train Epoch: 6 [1120/2444 (46%)]\tLoss: 0.499067\n",
            "Train Epoch: 6 [1280/2444 (52%)]\tLoss: 0.526912\n",
            "Train Epoch: 6 [1440/2444 (59%)]\tLoss: 0.476530\n",
            "Train Epoch: 6 [1600/2444 (65%)]\tLoss: 1.105271\n",
            "Train Epoch: 6 [1760/2444 (72%)]\tLoss: 0.952224\n",
            "Train Epoch: 6 [1920/2444 (78%)]\tLoss: 0.250388\n",
            "Train Epoch: 6 [2080/2444 (85%)]\tLoss: 0.769995\n",
            "Train Epoch: 6 [2240/2444 (92%)]\tLoss: 0.399892\n",
            "Train Epoch: 6 [2400/2444 (98%)]\tLoss: 0.130861\n",
            "epoch:6,loss:0.5206353377088223\n",
            "132 9\n",
            "\n",
            "Val set: Average loss: 0.8259, Accuracy: 95/132 (72%)\n",
            "\n",
            "2444 153\n",
            "Train Epoch: 7 [160/2444 (7%)]\tLoss: 0.438840\n",
            "Train Epoch: 7 [320/2444 (13%)]\tLoss: 0.359464\n",
            "Train Epoch: 7 [480/2444 (20%)]\tLoss: 0.185945\n",
            "Train Epoch: 7 [640/2444 (26%)]\tLoss: 0.282587\n",
            "Train Epoch: 7 [800/2444 (33%)]\tLoss: 0.586743\n",
            "Train Epoch: 7 [960/2444 (39%)]\tLoss: 0.160829\n",
            "Train Epoch: 7 [1120/2444 (46%)]\tLoss: 0.368781\n",
            "Train Epoch: 7 [1280/2444 (52%)]\tLoss: 0.684837\n",
            "Train Epoch: 7 [1440/2444 (59%)]\tLoss: 0.794184\n",
            "Train Epoch: 7 [1600/2444 (65%)]\tLoss: 0.710039\n",
            "Train Epoch: 7 [1760/2444 (72%)]\tLoss: 0.330835\n",
            "Train Epoch: 7 [1920/2444 (78%)]\tLoss: 0.392673\n",
            "Train Epoch: 7 [2080/2444 (85%)]\tLoss: 0.821804\n",
            "Train Epoch: 7 [2240/2444 (92%)]\tLoss: 0.414710\n",
            "Train Epoch: 7 [2400/2444 (98%)]\tLoss: 0.648441\n",
            "epoch:7,loss:0.3762640507490027\n",
            "132 9\n",
            "\n",
            "Val set: Average loss: 0.9257, Accuracy: 95/132 (72%)\n",
            "\n",
            "2444 153\n",
            "Train Epoch: 8 [160/2444 (7%)]\tLoss: 0.131255\n",
            "Train Epoch: 8 [320/2444 (13%)]\tLoss: 0.234956\n",
            "Train Epoch: 8 [480/2444 (20%)]\tLoss: 0.452947\n",
            "Train Epoch: 8 [640/2444 (26%)]\tLoss: 0.107409\n",
            "Train Epoch: 8 [800/2444 (33%)]\tLoss: 0.148567\n",
            "Train Epoch: 8 [960/2444 (39%)]\tLoss: 0.486432\n",
            "Train Epoch: 8 [1120/2444 (46%)]\tLoss: 0.261488\n",
            "Train Epoch: 8 [1280/2444 (52%)]\tLoss: 0.199696\n",
            "Train Epoch: 8 [1440/2444 (59%)]\tLoss: 0.215205\n",
            "Train Epoch: 8 [1600/2444 (65%)]\tLoss: 0.380454\n",
            "Train Epoch: 8 [1760/2444 (72%)]\tLoss: 0.133589\n",
            "Train Epoch: 8 [1920/2444 (78%)]\tLoss: 0.145675\n",
            "Train Epoch: 8 [2080/2444 (85%)]\tLoss: 0.251043\n",
            "Train Epoch: 8 [2240/2444 (92%)]\tLoss: 0.520300\n",
            "Train Epoch: 8 [2400/2444 (98%)]\tLoss: 0.515293\n",
            "epoch:8,loss:0.2932844525629204\n",
            "132 9\n",
            "\n",
            "Val set: Average loss: 0.8060, Accuracy: 97/132 (73%)\n",
            "\n",
            "2444 153\n",
            "Train Epoch: 9 [160/2444 (7%)]\tLoss: 0.690099\n",
            "Train Epoch: 9 [320/2444 (13%)]\tLoss: 0.625622\n",
            "Train Epoch: 9 [480/2444 (20%)]\tLoss: 0.327462\n",
            "Train Epoch: 9 [640/2444 (26%)]\tLoss: 0.264321\n",
            "Train Epoch: 9 [800/2444 (33%)]\tLoss: 0.232758\n",
            "Train Epoch: 9 [960/2444 (39%)]\tLoss: 0.346256\n",
            "Train Epoch: 9 [1120/2444 (46%)]\tLoss: 0.114502\n",
            "Train Epoch: 9 [1280/2444 (52%)]\tLoss: 0.063705\n",
            "Train Epoch: 9 [1440/2444 (59%)]\tLoss: 0.237402\n",
            "Train Epoch: 9 [1600/2444 (65%)]\tLoss: 0.361563\n",
            "Train Epoch: 9 [1760/2444 (72%)]\tLoss: 0.061235\n",
            "Train Epoch: 9 [1920/2444 (78%)]\tLoss: 0.433663\n",
            "Train Epoch: 9 [2080/2444 (85%)]\tLoss: 0.211026\n",
            "Train Epoch: 9 [2240/2444 (92%)]\tLoss: 0.227661\n",
            "Train Epoch: 9 [2400/2444 (98%)]\tLoss: 0.033692\n",
            "epoch:9,loss:0.2420666522522962\n",
            "132 9\n",
            "\n",
            "Val set: Average loss: 0.9191, Accuracy: 100/132 (76%)\n",
            "\n",
            "2444 153\n",
            "Train Epoch: 10 [160/2444 (7%)]\tLoss: 0.141808\n",
            "Train Epoch: 10 [320/2444 (13%)]\tLoss: 0.138185\n",
            "Train Epoch: 10 [480/2444 (20%)]\tLoss: 0.076066\n",
            "Train Epoch: 10 [640/2444 (26%)]\tLoss: 0.264245\n",
            "Train Epoch: 10 [800/2444 (33%)]\tLoss: 0.146399\n",
            "Train Epoch: 10 [960/2444 (39%)]\tLoss: 0.077054\n",
            "Train Epoch: 10 [1120/2444 (46%)]\tLoss: 0.080481\n",
            "Train Epoch: 10 [1280/2444 (52%)]\tLoss: 0.089636\n",
            "Train Epoch: 10 [1440/2444 (59%)]\tLoss: 0.213535\n",
            "Train Epoch: 10 [1600/2444 (65%)]\tLoss: 0.042604\n",
            "Train Epoch: 10 [1760/2444 (72%)]\tLoss: 0.043351\n",
            "Train Epoch: 10 [1920/2444 (78%)]\tLoss: 0.344643\n",
            "Train Epoch: 10 [2080/2444 (85%)]\tLoss: 0.173444\n",
            "Train Epoch: 10 [2240/2444 (92%)]\tLoss: 0.019825\n",
            "Train Epoch: 10 [2400/2444 (98%)]\tLoss: 0.024630\n",
            "epoch:10,loss:0.131471327846882\n",
            "132 9\n",
            "\n",
            "Val set: Average loss: 1.1876, Accuracy: 95/132 (72%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define training process\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()#set the mode to train mode, not actually training using this function\n",
        "    sum_loss = 0\n",
        "    total_num = len(train_loader.dataset)\n",
        "    print(total_num, len(train_loader))\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):#enumrate is a counter for the loop\n",
        "        data, target = Variable(data).to(device), Variable(target).to(device)#send data\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print_loss = loss.data.item()\n",
        "        sum_loss += print_loss\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
        "                       100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
        "    ave_loss = sum_loss / len(train_loader)\n",
        "    print('epoch:{},loss:{}'.format(epoch, ave_loss))\n",
        "\n",
        "\n",
        "# Verification process\n",
        "def val(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total_num = len(test_loader.dataset)\n",
        "    print(total_num, len(test_loader))\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = Variable(data).to(device), Variable(target).to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            _, pred = torch.max(output.data, 1)\n",
        "            correct += torch.sum(pred == target)\n",
        "            print_loss = loss.data.item()\n",
        "            test_loss += print_loss\n",
        "        correct = correct.data.item()\n",
        "        acc = correct / total_num\n",
        "        avgloss = test_loss / len(test_loader)\n",
        "        print('\\nVal set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            avgloss, correct, len(test_loader.dataset), 100 * acc))\n",
        "\n",
        "\n",
        "# train\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train(model_ft, DEVICE, train_loader, optimizer, epoch)\n",
        "    cosine_schedule.step()\n",
        "    val(model_ft, DEVICE, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "_IayLZyd8JEz"
      },
      "outputs": [],
      "source": [
        "torch.save(model_ft, 'model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I4KGuLZ3h_M"
      },
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "rUk-Aq0o3jmj"
      },
      "outputs": [],
      "source": [
        "classes = (\n",
        "     'daisy', 'dandelion', 'rose', 'sunflower', 'tulip'\n",
        ")\n",
        "     \n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "o3Bl6wuo3l6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d82c88f5-5dd4-4904-dad5-d0e6151ec95f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CoAtNet(\n",
              "  (s0): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): GELU(approximate=none)\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): GELU(approximate=none)\n",
              "    )\n",
              "  )\n",
              "  (s1): Sequential(\n",
              "    (0): MBConv(\n",
              "      (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (proj): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (conv): PreNorm(\n",
              "        (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fn): Sequential(\n",
              "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): GELU(approximate=none)\n",
              "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): GELU(approximate=none)\n",
              "          (6): SE(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc): Sequential(\n",
              "              (0): Linear(in_features=256, out_features=16, bias=False)\n",
              "              (1): GELU(approximate=none)\n",
              "              (2): Linear(in_features=16, out_features=256, bias=False)\n",
              "              (3): Sigmoid()\n",
              "            )\n",
              "          )\n",
              "          (7): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (8): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): MBConv(\n",
              "      (conv): PreNorm(\n",
              "        (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fn): Sequential(\n",
              "          (0): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): GELU(approximate=none)\n",
              "          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): GELU(approximate=none)\n",
              "          (6): SE(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc): Sequential(\n",
              "              (0): Linear(in_features=384, out_features=24, bias=False)\n",
              "              (1): GELU(approximate=none)\n",
              "              (2): Linear(in_features=24, out_features=384, bias=False)\n",
              "              (3): Sigmoid()\n",
              "            )\n",
              "          )\n",
              "          (7): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (8): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (s2): Sequential(\n",
              "    (0): MBConv(\n",
              "      (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (proj): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (conv): PreNorm(\n",
              "        (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fn): Sequential(\n",
              "          (0): Conv2d(96, 384, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): GELU(approximate=none)\n",
              "          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): GELU(approximate=none)\n",
              "          (6): SE(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc): Sequential(\n",
              "              (0): Linear(in_features=384, out_features=24, bias=False)\n",
              "              (1): GELU(approximate=none)\n",
              "              (2): Linear(in_features=24, out_features=384, bias=False)\n",
              "              (3): Sigmoid()\n",
              "            )\n",
              "          )\n",
              "          (7): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (8): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): MBConv(\n",
              "      (conv): PreNorm(\n",
              "        (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fn): Sequential(\n",
              "          (0): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): GELU(approximate=none)\n",
              "          (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
              "          (4): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): GELU(approximate=none)\n",
              "          (6): SE(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=48, bias=False)\n",
              "              (1): GELU(approximate=none)\n",
              "              (2): Linear(in_features=48, out_features=768, bias=False)\n",
              "              (3): Sigmoid()\n",
              "            )\n",
              "          )\n",
              "          (7): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (8): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): MBConv(\n",
              "      (conv): PreNorm(\n",
              "        (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fn): Sequential(\n",
              "          (0): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): GELU(approximate=none)\n",
              "          (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
              "          (4): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): GELU(approximate=none)\n",
              "          (6): SE(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=48, bias=False)\n",
              "              (1): GELU(approximate=none)\n",
              "              (2): Linear(in_features=48, out_features=768, bias=False)\n",
              "              (3): Sigmoid()\n",
              "            )\n",
              "          )\n",
              "          (7): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (8): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (s3): Sequential(\n",
              "    (0): Transformer(\n",
              "      (pool1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (pool2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (proj): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (attn): Sequential(\n",
              "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Attention(\n",
              "            (attend): Softmax(dim=-1)\n",
              "            (to_qkv): Linear(in_features=192, out_features=768, bias=False)\n",
              "            (to_out): Sequential(\n",
              "              (0): Linear(in_features=256, out_features=384, bias=True)\n",
              "              (1): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=14, iw=14)\n",
              "      )\n",
              "      (ff): Sequential(\n",
              "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): FeedForward(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=384, out_features=768, bias=True)\n",
              "              (1): GELU(approximate=none)\n",
              "              (2): Dropout(p=0.0, inplace=False)\n",
              "              (3): Linear(in_features=768, out_features=384, bias=True)\n",
              "              (4): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=14, iw=14)\n",
              "      )\n",
              "    )\n",
              "    (1): Transformer(\n",
              "      (attn): Sequential(\n",
              "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Attention(\n",
              "            (attend): Softmax(dim=-1)\n",
              "            (to_qkv): Linear(in_features=384, out_features=768, bias=False)\n",
              "            (to_out): Sequential(\n",
              "              (0): Linear(in_features=256, out_features=384, bias=True)\n",
              "              (1): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=14, iw=14)\n",
              "      )\n",
              "      (ff): Sequential(\n",
              "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): FeedForward(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "              (1): GELU(approximate=none)\n",
              "              (2): Dropout(p=0.0, inplace=False)\n",
              "              (3): Linear(in_features=1536, out_features=384, bias=True)\n",
              "              (4): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=14, iw=14)\n",
              "      )\n",
              "    )\n",
              "    (2): Transformer(\n",
              "      (attn): Sequential(\n",
              "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Attention(\n",
              "            (attend): Softmax(dim=-1)\n",
              "            (to_qkv): Linear(in_features=384, out_features=768, bias=False)\n",
              "            (to_out): Sequential(\n",
              "              (0): Linear(in_features=256, out_features=384, bias=True)\n",
              "              (1): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=14, iw=14)\n",
              "      )\n",
              "      (ff): Sequential(\n",
              "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): FeedForward(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "              (1): GELU(approximate=none)\n",
              "              (2): Dropout(p=0.0, inplace=False)\n",
              "              (3): Linear(in_features=1536, out_features=384, bias=True)\n",
              "              (4): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=14, iw=14)\n",
              "      )\n",
              "    )\n",
              "    (3): Transformer(\n",
              "      (attn): Sequential(\n",
              "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Attention(\n",
              "            (attend): Softmax(dim=-1)\n",
              "            (to_qkv): Linear(in_features=384, out_features=768, bias=False)\n",
              "            (to_out): Sequential(\n",
              "              (0): Linear(in_features=256, out_features=384, bias=True)\n",
              "              (1): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=14, iw=14)\n",
              "      )\n",
              "      (ff): Sequential(\n",
              "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): FeedForward(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "              (1): GELU(approximate=none)\n",
              "              (2): Dropout(p=0.0, inplace=False)\n",
              "              (3): Linear(in_features=1536, out_features=384, bias=True)\n",
              "              (4): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=14, iw=14)\n",
              "      )\n",
              "    )\n",
              "    (4): Transformer(\n",
              "      (attn): Sequential(\n",
              "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Attention(\n",
              "            (attend): Softmax(dim=-1)\n",
              "            (to_qkv): Linear(in_features=384, out_features=768, bias=False)\n",
              "            (to_out): Sequential(\n",
              "              (0): Linear(in_features=256, out_features=384, bias=True)\n",
              "              (1): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=14, iw=14)\n",
              "      )\n",
              "      (ff): Sequential(\n",
              "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): FeedForward(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "              (1): GELU(approximate=none)\n",
              "              (2): Dropout(p=0.0, inplace=False)\n",
              "              (3): Linear(in_features=1536, out_features=384, bias=True)\n",
              "              (4): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=14, iw=14)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (s4): Sequential(\n",
              "    (0): Transformer(\n",
              "      (pool1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (pool2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (proj): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (attn): Sequential(\n",
              "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Attention(\n",
              "            (attend): Softmax(dim=-1)\n",
              "            (to_qkv): Linear(in_features=384, out_features=768, bias=False)\n",
              "            (to_out): Sequential(\n",
              "              (0): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (1): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=7, iw=7)\n",
              "      )\n",
              "      (ff): Sequential(\n",
              "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): FeedForward(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=1536, bias=True)\n",
              "              (1): GELU(approximate=none)\n",
              "              (2): Dropout(p=0.0, inplace=False)\n",
              "              (3): Linear(in_features=1536, out_features=768, bias=True)\n",
              "              (4): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=7, iw=7)\n",
              "      )\n",
              "    )\n",
              "    (1): Transformer(\n",
              "      (attn): Sequential(\n",
              "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Attention(\n",
              "            (attend): Softmax(dim=-1)\n",
              "            (to_qkv): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (to_out): Sequential(\n",
              "              (0): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (1): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=7, iw=7)\n",
              "      )\n",
              "      (ff): Sequential(\n",
              "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): FeedForward(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU(approximate=none)\n",
              "              (2): Dropout(p=0.0, inplace=False)\n",
              "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (4): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=7, iw=7)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
              "  (fc): Linear(in_features=768, out_features=200, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "#load the model and put model in DEVICE\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = torch.load(\"model.pth\")\n",
        "model.eval()\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "DVdZvaD73qdG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46ac8d04-c967-4c85-c441-c95a169b4cc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Name: daisy.jpg, predict: daisy\n",
            "Image Name: rose.jpg, predict: rose\n",
            "Image Name: sunflower.jpg, predict: sunflower\n",
            "Image Name: tulip.jpg, predict: rose\n",
            "Image Name: daisy2.jpg, predict: daisy\n",
            "Image Name: daisy3.jpg, predict: daisy\n",
            "Image Name: rose2.jpg, predict: rose\n",
            "Image Name: rose3.jpg, predict: rose\n",
            "Image Name: sunflower2.jpg, predict: sunflower\n",
            "Image Name: sunflower3.jpg, predict: sunflower\n",
            "Image Name: tulip2.jpg, predict: tulip\n",
            "Image Name: tulip3.jpg, predict: tulip\n",
            "Image Name: dandelion.jpg, predict: dandelion\n",
            "Image Name: dandelion2.jpg, predict: dandelion\n",
            "Image Name: dandelion3.jpg, predict: dandelion\n",
            "Image Name: sunflower_dl.jpeg, predict: sunflower\n"
          ]
        }
      ],
      "source": [
        "#Read picture and predict category of picture\n",
        "path = 'drive/MyDrive/195A+BSeniorProjectGroupWorks/Datasets/test(christy)/'\n",
        "testList = os.listdir(path)\n",
        "for file in testList:\n",
        "    img = Image.open(path + file)\n",
        "    img = transform_test(img)\n",
        "    img.unsqueeze_(0)\n",
        "    img = Variable(img).to(DEVICE)\n",
        "    out = model(img)\n",
        "    # Predict\n",
        "    _, pred = torch.max(out.data, 1)\n",
        "    print('Image Name: {}, predict: {}'.format(file, classes[pred.data.item()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "rCUozSw6IMEQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebd13761-54e3-46a9-c770-debc1f5e6867"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Name: passion-flower1.jpg, predict: dandelion\n",
            "Image Name: paassion-flower2.jpg, predict: dandelion\n",
            "Image Name: california-poppy.jpg, predict: daisy\n",
            "Image Name: bougainvillea.jpg, predict: tulip\n",
            "Image Name: water-lily.jpg, predict: daisy\n",
            "Image Name: morning-glory.jpg, predict: rose\n",
            "Image Name: morning_glory.jpg, predict: dandelion\n",
            "Image Name: california-poppy1.jpg, predict: tulip\n",
            "Image Name: california-poppy2.jpg, predict: sunflower\n",
            "Image Name: foxglove.jpg, predict: rose\n",
            "Image Name: fangipani.jpg, predict: rose\n",
            "Image Name: oxeye-daisy.jpg, predict: daisy\n",
            "Image Name: asian-virginbower.jpg, predict: dandelion\n",
            "Image Name: bluebell.jpg, predict: dandelion\n",
            "Image Name: snowdrop.jpg, predict: dandelion\n",
            "Image Name: crocus.jpg, predict: tulip\n",
            "Image Name: ammi.jpg, predict: dandelion\n",
            "Image Name: rose.jpg, predict: rose\n",
            "Image Name: daisy.jpg, predict: daisy\n",
            "Image Name: calla.jpg, predict: tulip\n",
            "Image Name: bird-of-paradise.jpg, predict: rose\n",
            "Image Name: dandelion.jpg, predict: dandelion\n",
            "Image Name: aster1.jpg, predict: tulip\n",
            "Image Name: aster2.jpg, predict: rose\n",
            "Image Name: narcissus.jpg, predict: rose\n",
            "Image Name: muscari.jpg, predict: dandelion\n",
            "Image Name: protea1.jpg, predict: tulip\n",
            "Image Name: protea2.jpg, predict: rose\n"
          ]
        }
      ],
      "source": [
        "#Read picture and predict category of picture 2\n",
        "path = 'drive/MyDrive/195A+BSeniorProjectGroupWorks/Datasets/Flower_Dataset/test_set/'\n",
        "testList = os.listdir(path)\n",
        "for file in testList:\n",
        "    img = Image.open(path + file)\n",
        "    img = transform_test(img)\n",
        "    img.unsqueeze_(0)\n",
        "    img = Variable(img).to(DEVICE)\n",
        "    out = model(img)\n",
        "    # Predict\n",
        "    _, pred = torch.max(out.data, 1)\n",
        "    print('Image Name: {}, predict: {}'.format(file, classes[pred.data.item()]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qMt3cx24gl9"
      },
      "source": [
        "Note: larger dataset yields worse result\n",
        "Why?\n",
        "*Figure this out and make it a novelty point* \n",
        "\n",
        "Priority: expand dataset"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}